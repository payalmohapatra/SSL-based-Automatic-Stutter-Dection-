{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "# First things first! Set a seed for reproducibility.\n",
    "# https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "##################################################################################################\n",
    "writer = SummaryWriter()\n",
    "writer = SummaryWriter(\"wav2vec_base_model\")\n",
    "writer = SummaryWriter(comment=\"2D conv layer architecture from edge impulse ; lr = 1e-5; 100 epochs; 10 mins data\")\n",
    "##################################################################################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device available is', device)\n",
    "# wav2vec2.0\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "print(\"Sample Rate of model:\", bundle.sample_rate)\n",
    "\n",
    "model_wav2vec = bundle.get_model().to(device)\n",
    "## Convert audio to numpy to wav2vec feature encodings\n",
    "def conv_audio_data (filename) :\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    waveform = waveform.to(device)\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        print('Mismatched sample rate')\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    emission, _ = model_wav2vec(waveform)\n",
    "    emission = emission.cpu().detach().numpy()\n",
    "    return emission\n",
    "\n",
    "x_f = []\n",
    "y_f = []\n",
    "x_s = []\n",
    "y_s = []\n",
    "# get all stutter data\n",
    "#path_stutter  = \"/home/payal/wav2vec/Dataset/all_stutter/\"\n",
    "path_stutter  = \"Dataset/all_stutter/\"\n",
    "files_stutter = os.listdir(path_stutter)\n",
    "\n",
    "for filename in glob.glob(os.path.join(path_stutter, '*.wav')):\n",
    "    stutter_np = conv_audio_data(filename)\n",
    "    x_s.append(stutter_np)\n",
    "    y_s.append(1)\n",
    "\n",
    "# get all fluent data\n",
    "discarded = 0\n",
    "#FIXME :: How can I avoid discarding the mismatched samples?\n",
    "#path_fluent  = \"/home/payal/wav2vec/Dataset/all_fluent/\"\n",
    "path_fluent  = \"/Dataset/all_fluent/\"\n",
    "files_fluent = os.listdir(path_fluent)\n",
    "for filename in glob.glob(os.path.join(path_fluent, '*.wav')):\n",
    "    fluent_np = conv_audio_data(filename)\n",
    "    # fluent_np --> (1, 149, 768)\n",
    "    if ((np.shape(fluent_np)[0] != 1) |(np.shape(fluent_np)[1] != 149) | (np.shape(fluent_np)[2] != 768)) :\n",
    "        discarded += 1\n",
    "    else:\n",
    "        x_f.append(fluent_np)\n",
    "        y_f.append(0)\n",
    "\n",
    "# Shuffle all data within a class so that we have samples from all podcasts.\n",
    "random.shuffle(x_f)\n",
    "random.shuffle(x_s)\n",
    "\n",
    "# 100 samples each for 10 mins training\n",
    "x_f_train = x_f[0:100]\n",
    "y_f_train = y_f[0:100]\n",
    "x_s_train = x_s[0:100]\n",
    "y_s_train = y_s[0:100]\n",
    "\n",
    "\n",
    "# 100 samples each for 10 mins testing\n",
    "x_f_test = x_f[-100:-1]\n",
    "y_f_test = y_f[-100:-1]\n",
    "x_s_test = x_s[-100:-1]\n",
    "y_s_test = y_s[-100:-1]\n",
    "\n",
    "# FIXME :: Shuffle this later on so that all classesa re not given sequentially for training\n",
    "x_train = x_f_train + x_s_train\n",
    "y_train = y_f_train + y_s_train\n",
    "\n",
    "x_test = x_f_test + x_s_test\n",
    "y_test = y_f_test + y_s_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper parameters\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "# split data and translate to dataloader\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)\n",
    "\n",
    "n_samples_train = np.shape(x_train)[0]\n",
    "n_samples_test = np.shape(x_test)[0]\n",
    "print('Number of samples to train = ', n_samples_train)\n",
    "print('Number of samples to test = ', n_samples_test)\n",
    "\n",
    "class AudioDataset(Dataset) :\n",
    "    def __init__(self,x,y, n_samples) :\n",
    "        # data loading\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index) :\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self) :    \n",
    "        return self.n_samples      \n",
    "\n",
    "train_dataset = AudioDataset(x_train,y_train,n_samples_train)\n",
    "test_dataset = AudioDataset(x_test,y_test,n_samples_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "\n",
    "class StutterNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(StutterNet, self).__init__()\n",
    "        # input shape = (batch_size, 1, 149,768)\n",
    "        # in_channels is batch size\n",
    "        self.layer1 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            torch.nn.Dropout(p=0.5)\n",
    "        )\n",
    "        # input size = (batch_size, 8, 74, 384)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=1, stride=2),\n",
    "            torch.nn.Dropout(p=0.5)\n",
    "        )\n",
    "        # input size = (batch_size, 16, 37, 192)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16* 37* 192,2, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('Before Layer1',np.shape(x))\n",
    "        out = self.layer1(x)\n",
    "        #print('After layer 1',np.shape(out))\n",
    "        out = self.layer2(out)\n",
    "        #print('After layer 2',np.shape(out))\n",
    "        out  = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        #print('After final ',np.shape(out))\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "model = StutterNet(batch_size).to(device)\n",
    "print(model)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#weighted loss\n",
    "#criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1, 3]).to(device)) # Class 0 is 75% of the total dataset \n",
    "#criterion = nn.LogSoftmax()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    n_correct = 0\n",
    "    for i, (features, labels) in enumerate(train_loader):  \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.reshape(labels,(np.shape(labels)[0],))\n",
    "        labels = labels.to(torch.int64)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute Training Accuracy\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct = (labels == predicted_labels).sum()\n",
    "        acc = 100.0 * n_correct / outputs.shape[0]\n",
    "        # visualisation\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)  \n",
    "        writer.add_scalar(\"Accuracy/train\", acc, epoch)  \n",
    "        \n",
    "        if (i+1) % 2 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}, Acc : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "   n_correct = 0\n",
    "   # Compute F1 score, precision and recall\n",
    "   predicted_stutter = 0\n",
    "   labels_stutter = 0\n",
    "   correct_stutter = 0\n",
    "   i = 0\n",
    "   final_label = []\n",
    "   final_predicted = []\n",
    "   for features, labels in test_loader:\n",
    "    #    print(labels)\n",
    "    #    print(np.shape(features))\n",
    "       features = features.to(device)\n",
    "       labels = labels.to(device)\n",
    "       outputs = model(features)\n",
    "       print(np.shape(outputs))\n",
    "       # max returns (value ,index)\n",
    "       _, predicted = torch.max(outputs.data, 1)\n",
    "    #    print(predicted)\n",
    "    #    print(np.shape(predicted))\n",
    "       label = torch.transpose(predicted, -1, 0)\n",
    "       predicted = torch.reshape(predicted,(outputs.shape[0],1))\n",
    "       i = i+1\n",
    "    #    print(n_correct)\n",
    "       final_label.append(label)\n",
    "       final_predicted.append(predicted)\n",
    "       for i in range (0, outputs.shape[0]) :\n",
    "               # F1 score for stutter\n",
    "            if (predicted[i] == 1) :\n",
    "                predicted_stutter +=1\n",
    "            if (labels[i] == 1) :\n",
    "                labels_stutter +=1   \n",
    "            if ((predicted[i] == 1) & (labels[i] == 1)):\n",
    "                correct_stutter +=1\n",
    "            if (predicted[i] == labels[i]) :\n",
    "                n_correct = n_correct + 1\n",
    "\n",
    "\n",
    "\n",
    "acc_test = 100*n_correct/n_samples_test\n",
    "# print(f'Accuracy of the network on test dataset is : {acc_test} %')\n",
    "recall = correct_stutter/ labels_stutter\n",
    "precision = correct_stutter / predicted_stutter\n",
    "f1_score = 2 * precision * recall / (precision + recall)    \n",
    "# print(f'Precision of the network on test dataset is : {precision}')\n",
    "# print(f'Recall of the network on test dataset is : {recall}')\n",
    "print(f'F1 Score of the network on test dataset is : {f1_score}')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_label)\n",
    "print('-------------------------------------')\n",
    "print('Predicted')\n",
    "print(final_predicted)\n",
    "print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aede6801b29297ca4753447bcd9fb52ef6084259d335c2409ba40d2da343a20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
