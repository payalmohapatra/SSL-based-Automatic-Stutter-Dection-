{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available is cuda\n",
      "Sample Rate of model: 16000\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "# First things first! Set a seed for reproducibility.\n",
    "# https://www.cs.mcgill.ca/~ksinha4/practices_for_reproducibility/\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "##################################################################################################\n",
    "writer = SummaryWriter()\n",
    "writer = SummaryWriter(\"wav2vec_base_model_large\")\n",
    "writer = SummaryWriter(comment=\"2D conv layer; large model ; lr = 1e-8; 100 epochs; 10 mins data\")\n",
    "##################################################################################################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device available is', device)\n",
    "# wav2vec2.0\n",
    "#bundle = torchaudio.pipelines.WAV2VEC2_LARGE_LV60K\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_LARGE\n",
    "print(\"Sample Rate of model:\", bundle.sample_rate)\n",
    "\n",
    "model_wav2vec = bundle.get_model().to(device)\n",
    "## Convert audio to numpy to wav2vec feature encodings\n",
    "def conv_audio_data (filename) :\n",
    "    waveform, sample_rate = torchaudio.load(filename)\n",
    "    waveform = waveform.to(device)\n",
    "    if sample_rate != bundle.sample_rate:\n",
    "        print('Mismatched sample rate')\n",
    "        waveform = torchaudio.functional.resample(waveform, sample_rate, bundle.sample_rate)\n",
    "    emission, _ = model_wav2vec(waveform)\n",
    "    emission = emission.cpu().detach().numpy()\n",
    "    return emission\n",
    "\n",
    "x_f = []\n",
    "y_f = []\n",
    "x_s = []\n",
    "y_s = []\n",
    "# get all stutter data\n",
    "path_stutter  = \"Dataset/all_stutter/\"\n",
    "files_stutter = os.listdir(path_stutter)\n",
    "\n",
    "for filename in glob.glob(os.path.join(path_stutter, '*.wav')):\n",
    "    stutter_np = conv_audio_data(filename)\n",
    "    x_s.append(stutter_np)\n",
    "    y_s.append(1)\n",
    "\n",
    "# get all fluent data\n",
    "discarded = 0\n",
    "#FIXME :: How can I avoid discarding the mismatched samples?\n",
    "path_fluent  = \"Dataset/all_fluent/\"\n",
    "files_fluent = os.listdir(path_fluent)\n",
    "for filename in glob.glob(os.path.join(path_fluent, '*.wav')):\n",
    "    fluent_np = conv_audio_data(filename)\n",
    "    # fluent_np --> (1, 149, 1024)\n",
    "    if ((np.shape(fluent_np)[0] != 1) |(np.shape(fluent_np)[1] != 149) | (np.shape(fluent_np)[2] != 1024)) :\n",
    "        discarded += 1\n",
    "    else:\n",
    "        x_f.append(fluent_np)\n",
    "        y_f.append(0)\n",
    "\n",
    "# Shuffle all data within a class so that we have samples from all podcasts.\n",
    "random.shuffle(x_f)\n",
    "random.shuffle(x_s)\n",
    "\n",
    "# 100 samples each for 10 mins training\n",
    "x_f_train = x_f[0:100]\n",
    "y_f_train = y_f[0:100]\n",
    "x_s_train = x_s[0:100]\n",
    "y_s_train = y_s[0:100]\n",
    "\n",
    "\n",
    "# 100 samples each for 10 mins testing\n",
    "x_f_test = x_f[-100:-1]\n",
    "y_f_test = y_f[-100:-1]\n",
    "x_s_test = x_s[-100:-1]\n",
    "y_s_test = y_s[-100:-1]\n",
    "\n",
    "# FIXME :: Shuffle this later on so that all classesa re not given sequentially for training\n",
    "x_train = x_f_train + x_s_train\n",
    "y_train = y_f_train + y_s_train\n",
    "\n",
    "x_test = x_f_test + x_s_test\n",
    "y_test = y_f_test + y_s_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples to train =  200\n",
      "Number of samples to test =  198\n"
     ]
    }
   ],
   "source": [
    "## Hyper parameters\n",
    "batch_size = 64\n",
    "num_epochs = 500\n",
    "#learning_rate = 0.0001\n",
    "learning_rate = 1e-5\n",
    "\n",
    "\n",
    "# split data and translate to dataloader\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=123)\n",
    "\n",
    "n_samples_train = np.shape(x_train)[0]\n",
    "n_samples_test = np.shape(x_test)[0]\n",
    "print('Number of samples to train = ', n_samples_train)\n",
    "print('Number of samples to test = ', n_samples_test)\n",
    "\n",
    "class AudioDataset(Dataset) :\n",
    "    def __init__(self,x,y, n_samples) :\n",
    "        # data loading\n",
    "        self.x = x\n",
    "        self.y = y \n",
    "        self.n_samples = n_samples\n",
    "        \n",
    "        \n",
    "    def __getitem__(self,index) :\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self) :    \n",
    "        return self.n_samples      \n",
    "\n",
    "train_dataset = AudioDataset(x_train,y_train,n_samples_train)\n",
    "test_dataset = AudioDataset(x_test,y_test,n_samples_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StutterNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=1, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=77824, out_features=2, bias=True)\n",
      ")\n",
      "Epoch [1/500], Step [2/4], Loss: 0.6955, Acc : 46.875%\n",
      "Epoch [1/500], Step [4/4], Loss: 0.6933, Acc : 50.0%\n",
      "Epoch [2/500], Step [2/4], Loss: 0.6923, Acc : 65.625%\n",
      "Epoch [2/500], Step [4/4], Loss: 0.6907, Acc : 75.0%\n",
      "Epoch [3/500], Step [2/4], Loss: 0.6933, Acc : 50.0%\n",
      "Epoch [3/500], Step [4/4], Loss: 0.6759, Acc : 87.5%\n",
      "Epoch [4/500], Step [2/4], Loss: 0.6902, Acc : 54.6875%\n",
      "Epoch [4/500], Step [4/4], Loss: 0.6945, Acc : 50.0%\n",
      "Epoch [5/500], Step [2/4], Loss: 0.6857, Acc : 57.8125%\n",
      "Epoch [5/500], Step [4/4], Loss: 0.6948, Acc : 50.0%\n",
      "Epoch [6/500], Step [2/4], Loss: 0.6930, Acc : 51.5625%\n",
      "Epoch [6/500], Step [4/4], Loss: 0.6949, Acc : 50.0%\n",
      "Epoch [7/500], Step [2/4], Loss: 0.6927, Acc : 51.5625%\n",
      "Epoch [7/500], Step [4/4], Loss: 0.6941, Acc : 50.0%\n",
      "Epoch [8/500], Step [2/4], Loss: 0.6886, Acc : 56.25%\n",
      "Epoch [8/500], Step [4/4], Loss: 0.7022, Acc : 37.5%\n",
      "Epoch [9/500], Step [2/4], Loss: 0.6924, Acc : 51.5625%\n",
      "Epoch [9/500], Step [4/4], Loss: 0.6924, Acc : 50.0%\n",
      "Epoch [10/500], Step [2/4], Loss: 0.6927, Acc : 53.125%\n",
      "Epoch [10/500], Step [4/4], Loss: 0.6986, Acc : 25.0%\n",
      "Epoch [11/500], Step [2/4], Loss: 0.6937, Acc : 45.3125%\n",
      "Epoch [11/500], Step [4/4], Loss: 0.6960, Acc : 25.0%\n",
      "Epoch [12/500], Step [2/4], Loss: 0.6935, Acc : 45.3125%\n",
      "Epoch [12/500], Step [4/4], Loss: 0.6889, Acc : 62.5%\n",
      "Epoch [13/500], Step [2/4], Loss: 0.6955, Acc : 46.875%\n",
      "Epoch [13/500], Step [4/4], Loss: 0.7048, Acc : 37.5%\n",
      "Epoch [14/500], Step [2/4], Loss: 0.6938, Acc : 50.0%\n",
      "Epoch [14/500], Step [4/4], Loss: 0.7038, Acc : 37.5%\n",
      "Epoch [15/500], Step [2/4], Loss: 0.6901, Acc : 54.6875%\n",
      "Epoch [15/500], Step [4/4], Loss: 0.7069, Acc : 25.0%\n",
      "Epoch [16/500], Step [2/4], Loss: 0.6945, Acc : 42.1875%\n",
      "Epoch [16/500], Step [4/4], Loss: 0.6938, Acc : 37.5%\n",
      "Epoch [17/500], Step [2/4], Loss: 0.6933, Acc : 46.875%\n",
      "Epoch [17/500], Step [4/4], Loss: 0.7020, Acc : 25.0%\n",
      "Epoch [18/500], Step [2/4], Loss: 0.6955, Acc : 37.5%\n",
      "Epoch [18/500], Step [4/4], Loss: 0.6937, Acc : 37.5%\n",
      "Epoch [19/500], Step [2/4], Loss: 0.6929, Acc : 45.3125%\n",
      "Epoch [19/500], Step [4/4], Loss: 0.6951, Acc : 37.5%\n",
      "Epoch [20/500], Step [2/4], Loss: 0.6916, Acc : 53.125%\n",
      "Epoch [20/500], Step [4/4], Loss: 0.6876, Acc : 75.0%\n",
      "Epoch [21/500], Step [2/4], Loss: 0.6894, Acc : 57.8125%\n",
      "Epoch [21/500], Step [4/4], Loss: 0.6934, Acc : 50.0%\n",
      "Epoch [22/500], Step [2/4], Loss: 0.6880, Acc : 57.8125%\n",
      "Epoch [22/500], Step [4/4], Loss: 0.7088, Acc : 25.0%\n",
      "Epoch [23/500], Step [2/4], Loss: 0.6965, Acc : 42.1875%\n",
      "Epoch [23/500], Step [4/4], Loss: 0.6801, Acc : 87.5%\n",
      "Epoch [24/500], Step [2/4], Loss: 0.6934, Acc : 48.4375%\n",
      "Epoch [24/500], Step [4/4], Loss: 0.6929, Acc : 50.0%\n",
      "Epoch [25/500], Step [2/4], Loss: 0.6947, Acc : 46.875%\n",
      "Epoch [25/500], Step [4/4], Loss: 0.7012, Acc : 37.5%\n",
      "Epoch [26/500], Step [2/4], Loss: 0.6932, Acc : 48.4375%\n",
      "Epoch [26/500], Step [4/4], Loss: 0.6835, Acc : 75.0%\n",
      "Epoch [27/500], Step [2/4], Loss: 0.6927, Acc : 50.0%\n",
      "Epoch [27/500], Step [4/4], Loss: 0.6977, Acc : 37.5%\n",
      "Epoch [28/500], Step [2/4], Loss: 0.6943, Acc : 43.75%\n",
      "Epoch [28/500], Step [4/4], Loss: 0.6970, Acc : 37.5%\n",
      "Epoch [29/500], Step [2/4], Loss: 0.6925, Acc : 51.5625%\n",
      "Epoch [29/500], Step [4/4], Loss: 0.6938, Acc : 25.0%\n",
      "Epoch [30/500], Step [2/4], Loss: 0.6925, Acc : 50.0%\n",
      "Epoch [30/500], Step [4/4], Loss: 0.6953, Acc : 25.0%\n",
      "Epoch [31/500], Step [2/4], Loss: 0.6915, Acc : 64.0625%\n",
      "Epoch [31/500], Step [4/4], Loss: 0.6945, Acc : 37.5%\n",
      "Epoch [32/500], Step [2/4], Loss: 0.6919, Acc : 60.9375%\n",
      "Epoch [32/500], Step [4/4], Loss: 0.6961, Acc : 25.0%\n",
      "Epoch [33/500], Step [2/4], Loss: 0.6920, Acc : 50.0%\n",
      "Epoch [33/500], Step [4/4], Loss: 0.6961, Acc : 37.5%\n",
      "Epoch [34/500], Step [2/4], Loss: 0.6883, Acc : 57.8125%\n",
      "Epoch [34/500], Step [4/4], Loss: 0.6990, Acc : 37.5%\n",
      "Epoch [35/500], Step [2/4], Loss: 0.6935, Acc : 46.875%\n",
      "Epoch [35/500], Step [4/4], Loss: 0.6882, Acc : 62.5%\n",
      "Epoch [36/500], Step [2/4], Loss: 0.6947, Acc : 42.1875%\n",
      "Epoch [36/500], Step [4/4], Loss: 0.6876, Acc : 62.5%\n",
      "Epoch [37/500], Step [2/4], Loss: 0.6942, Acc : 45.3125%\n",
      "Epoch [37/500], Step [4/4], Loss: 0.6992, Acc : 37.5%\n",
      "Epoch [38/500], Step [2/4], Loss: 0.6891, Acc : 57.8125%\n",
      "Epoch [38/500], Step [4/4], Loss: 0.6960, Acc : 37.5%\n",
      "Epoch [39/500], Step [2/4], Loss: 0.6931, Acc : 42.1875%\n",
      "Epoch [39/500], Step [4/4], Loss: 0.6934, Acc : 50.0%\n",
      "Epoch [40/500], Step [2/4], Loss: 0.6921, Acc : 57.8125%\n",
      "Epoch [40/500], Step [4/4], Loss: 0.6895, Acc : 75.0%\n",
      "Epoch [41/500], Step [2/4], Loss: 0.6960, Acc : 40.625%\n",
      "Epoch [41/500], Step [4/4], Loss: 0.6914, Acc : 50.0%\n",
      "Epoch [42/500], Step [2/4], Loss: 0.6925, Acc : 50.0%\n",
      "Epoch [42/500], Step [4/4], Loss: 0.6827, Acc : 62.5%\n",
      "Epoch [43/500], Step [2/4], Loss: 0.6968, Acc : 45.3125%\n",
      "Epoch [43/500], Step [4/4], Loss: 0.6838, Acc : 62.5%\n",
      "Epoch [44/500], Step [2/4], Loss: 0.6863, Acc : 56.25%\n",
      "Epoch [44/500], Step [4/4], Loss: 0.7079, Acc : 37.5%\n",
      "Epoch [45/500], Step [2/4], Loss: 0.7017, Acc : 42.1875%\n",
      "Epoch [45/500], Step [4/4], Loss: 0.7155, Acc : 25.0%\n",
      "Epoch [46/500], Step [2/4], Loss: 0.6898, Acc : 53.125%\n",
      "Epoch [46/500], Step [4/4], Loss: 0.6901, Acc : 62.5%\n",
      "Epoch [47/500], Step [2/4], Loss: 0.6912, Acc : 60.9375%\n",
      "Epoch [47/500], Step [4/4], Loss: 0.6911, Acc : 50.0%\n",
      "Epoch [48/500], Step [2/4], Loss: 0.6916, Acc : 59.375%\n",
      "Epoch [48/500], Step [4/4], Loss: 0.6932, Acc : 50.0%\n",
      "Epoch [49/500], Step [2/4], Loss: 0.6917, Acc : 57.8125%\n",
      "Epoch [49/500], Step [4/4], Loss: 0.6863, Acc : 87.5%\n",
      "Epoch [50/500], Step [2/4], Loss: 0.6974, Acc : 37.5%\n",
      "Epoch [50/500], Step [4/4], Loss: 0.6973, Acc : 37.5%\n",
      "Epoch [51/500], Step [2/4], Loss: 0.6923, Acc : 50.0%\n",
      "Epoch [51/500], Step [4/4], Loss: 0.6848, Acc : 62.5%\n",
      "Epoch [52/500], Step [2/4], Loss: 0.6886, Acc : 54.6875%\n",
      "Epoch [52/500], Step [4/4], Loss: 0.7104, Acc : 25.0%\n",
      "Epoch [53/500], Step [2/4], Loss: 0.6892, Acc : 54.6875%\n",
      "Epoch [53/500], Step [4/4], Loss: 0.6972, Acc : 37.5%\n",
      "Epoch [54/500], Step [2/4], Loss: 0.6911, Acc : 57.8125%\n",
      "Epoch [54/500], Step [4/4], Loss: 0.6895, Acc : 75.0%\n",
      "Epoch [55/500], Step [2/4], Loss: 0.6913, Acc : 51.5625%\n",
      "Epoch [55/500], Step [4/4], Loss: 0.6879, Acc : 62.5%\n",
      "Epoch [56/500], Step [2/4], Loss: 0.6940, Acc : 45.3125%\n",
      "Epoch [56/500], Step [4/4], Loss: 0.7016, Acc : 37.5%\n",
      "Epoch [57/500], Step [2/4], Loss: 0.6931, Acc : 48.4375%\n",
      "Epoch [57/500], Step [4/4], Loss: 0.7061, Acc : 25.0%\n",
      "Epoch [58/500], Step [2/4], Loss: 0.6900, Acc : 53.125%\n",
      "Epoch [58/500], Step [4/4], Loss: 0.6889, Acc : 62.5%\n",
      "Epoch [59/500], Step [2/4], Loss: 0.6908, Acc : 68.75%\n",
      "Epoch [59/500], Step [4/4], Loss: 0.6887, Acc : 87.5%\n",
      "Epoch [60/500], Step [2/4], Loss: 0.6905, Acc : 54.6875%\n",
      "Epoch [60/500], Step [4/4], Loss: 0.6950, Acc : 37.5%\n",
      "Epoch [61/500], Step [2/4], Loss: 0.6908, Acc : 50.0%\n",
      "Epoch [61/500], Step [4/4], Loss: 0.6874, Acc : 62.5%\n",
      "Epoch [62/500], Step [2/4], Loss: 0.6921, Acc : 46.875%\n",
      "Epoch [62/500], Step [4/4], Loss: 0.6859, Acc : 62.5%\n",
      "Epoch [63/500], Step [2/4], Loss: 0.6872, Acc : 54.6875%\n",
      "Epoch [63/500], Step [4/4], Loss: 0.7008, Acc : 37.5%\n",
      "Epoch [64/500], Step [2/4], Loss: 0.6861, Acc : 57.8125%\n",
      "Epoch [64/500], Step [4/4], Loss: 0.6983, Acc : 37.5%\n",
      "Epoch [65/500], Step [2/4], Loss: 0.6881, Acc : 54.6875%\n",
      "Epoch [65/500], Step [4/4], Loss: 0.6903, Acc : 50.0%\n",
      "Epoch [66/500], Step [2/4], Loss: 0.6916, Acc : 53.125%\n",
      "Epoch [66/500], Step [4/4], Loss: 0.6916, Acc : 62.5%\n",
      "Epoch [67/500], Step [2/4], Loss: 0.6903, Acc : 60.9375%\n",
      "Epoch [67/500], Step [4/4], Loss: 0.6956, Acc : 50.0%\n",
      "Epoch [68/500], Step [2/4], Loss: 0.6918, Acc : 53.125%\n",
      "Epoch [68/500], Step [4/4], Loss: 0.6848, Acc : 75.0%\n",
      "Epoch [69/500], Step [2/4], Loss: 0.6991, Acc : 37.5%\n",
      "Epoch [69/500], Step [4/4], Loss: 0.6787, Acc : 62.5%\n",
      "Epoch [70/500], Step [2/4], Loss: 0.6975, Acc : 45.3125%\n",
      "Epoch [70/500], Step [4/4], Loss: 0.6855, Acc : 50.0%\n",
      "Epoch [71/500], Step [2/4], Loss: 0.6942, Acc : 48.4375%\n",
      "Epoch [71/500], Step [4/4], Loss: 0.7102, Acc : 37.5%\n",
      "Epoch [72/500], Step [2/4], Loss: 0.6930, Acc : 48.4375%\n",
      "Epoch [72/500], Step [4/4], Loss: 0.7047, Acc : 37.5%\n",
      "Epoch [73/500], Step [2/4], Loss: 0.6877, Acc : 54.6875%\n",
      "Epoch [73/500], Step [4/4], Loss: 0.6854, Acc : 50.0%\n",
      "Epoch [74/500], Step [2/4], Loss: 0.6893, Acc : 60.9375%\n",
      "Epoch [74/500], Step [4/4], Loss: 0.6980, Acc : 50.0%\n",
      "Epoch [75/500], Step [2/4], Loss: 0.6877, Acc : 67.1875%\n",
      "Epoch [75/500], Step [4/4], Loss: 0.6953, Acc : 50.0%\n",
      "Epoch [76/500], Step [2/4], Loss: 0.6899, Acc : 64.0625%\n",
      "Epoch [76/500], Step [4/4], Loss: 0.6869, Acc : 75.0%\n",
      "Epoch [77/500], Step [2/4], Loss: 0.6917, Acc : 54.6875%\n",
      "Epoch [77/500], Step [4/4], Loss: 0.6904, Acc : 37.5%\n",
      "Epoch [78/500], Step [2/4], Loss: 0.6910, Acc : 51.5625%\n",
      "Epoch [78/500], Step [4/4], Loss: 0.6915, Acc : 50.0%\n",
      "Epoch [79/500], Step [2/4], Loss: 0.6898, Acc : 60.9375%\n",
      "Epoch [79/500], Step [4/4], Loss: 0.6925, Acc : 75.0%\n",
      "Epoch [80/500], Step [2/4], Loss: 0.6882, Acc : 60.9375%\n",
      "Epoch [80/500], Step [4/4], Loss: 0.6894, Acc : 50.0%\n",
      "Epoch [81/500], Step [2/4], Loss: 0.6907, Acc : 59.375%\n",
      "Epoch [81/500], Step [4/4], Loss: 0.6896, Acc : 62.5%\n",
      "Epoch [82/500], Step [2/4], Loss: 0.6873, Acc : 57.8125%\n",
      "Epoch [82/500], Step [4/4], Loss: 0.6837, Acc : 62.5%\n",
      "Epoch [83/500], Step [2/4], Loss: 0.6897, Acc : 48.4375%\n",
      "Epoch [83/500], Step [4/4], Loss: 0.6945, Acc : 37.5%\n",
      "Epoch [84/500], Step [2/4], Loss: 0.6910, Acc : 50.0%\n",
      "Epoch [84/500], Step [4/4], Loss: 0.6976, Acc : 37.5%\n",
      "Epoch [85/500], Step [2/4], Loss: 0.6865, Acc : 64.0625%\n",
      "Epoch [85/500], Step [4/4], Loss: 0.6820, Acc : 75.0%\n",
      "Epoch [86/500], Step [2/4], Loss: 0.6870, Acc : 71.875%\n",
      "Epoch [86/500], Step [4/4], Loss: 0.6935, Acc : 37.5%\n",
      "Epoch [87/500], Step [2/4], Loss: 0.6879, Acc : 59.375%\n",
      "Epoch [87/500], Step [4/4], Loss: 0.6914, Acc : 37.5%\n",
      "Epoch [88/500], Step [2/4], Loss: 0.6908, Acc : 59.375%\n",
      "Epoch [88/500], Step [4/4], Loss: 0.6951, Acc : 50.0%\n",
      "Epoch [89/500], Step [2/4], Loss: 0.6882, Acc : 62.5%\n",
      "Epoch [89/500], Step [4/4], Loss: 0.7022, Acc : 37.5%\n",
      "Epoch [90/500], Step [2/4], Loss: 0.6889, Acc : 62.5%\n",
      "Epoch [90/500], Step [4/4], Loss: 0.6980, Acc : 37.5%\n",
      "Epoch [91/500], Step [2/4], Loss: 0.6889, Acc : 68.75%\n",
      "Epoch [91/500], Step [4/4], Loss: 0.6896, Acc : 62.5%\n",
      "Epoch [92/500], Step [2/4], Loss: 0.6883, Acc : 67.1875%\n",
      "Epoch [92/500], Step [4/4], Loss: 0.6833, Acc : 75.0%\n",
      "Epoch [93/500], Step [2/4], Loss: 0.6887, Acc : 62.5%\n",
      "Epoch [93/500], Step [4/4], Loss: 0.6953, Acc : 37.5%\n",
      "Epoch [94/500], Step [2/4], Loss: 0.6902, Acc : 48.4375%\n",
      "Epoch [94/500], Step [4/4], Loss: 0.6824, Acc : 50.0%\n",
      "Epoch [95/500], Step [2/4], Loss: 0.6864, Acc : 51.5625%\n",
      "Epoch [95/500], Step [4/4], Loss: 0.6630, Acc : 87.5%\n",
      "Epoch [96/500], Step [2/4], Loss: 0.6856, Acc : 53.125%\n",
      "Epoch [96/500], Step [4/4], Loss: 0.7027, Acc : 37.5%\n",
      "Epoch [97/500], Step [2/4], Loss: 0.6872, Acc : 51.5625%\n",
      "Epoch [97/500], Step [4/4], Loss: 0.7105, Acc : 25.0%\n",
      "Epoch [98/500], Step [2/4], Loss: 0.6841, Acc : 54.6875%\n",
      "Epoch [98/500], Step [4/4], Loss: 0.6890, Acc : 62.5%\n",
      "Epoch [99/500], Step [2/4], Loss: 0.6875, Acc : 62.5%\n",
      "Epoch [99/500], Step [4/4], Loss: 0.6916, Acc : 62.5%\n",
      "Epoch [100/500], Step [2/4], Loss: 0.6868, Acc : 60.9375%\n",
      "Epoch [100/500], Step [4/4], Loss: 0.6851, Acc : 62.5%\n",
      "Epoch [101/500], Step [2/4], Loss: 0.6865, Acc : 54.6875%\n",
      "Epoch [101/500], Step [4/4], Loss: 0.6796, Acc : 62.5%\n",
      "Epoch [102/500], Step [2/4], Loss: 0.6820, Acc : 54.6875%\n",
      "Epoch [102/500], Step [4/4], Loss: 0.6982, Acc : 37.5%\n",
      "Epoch [103/500], Step [2/4], Loss: 0.6963, Acc : 40.625%\n",
      "Epoch [103/500], Step [4/4], Loss: 0.6885, Acc : 62.5%\n",
      "Epoch [104/500], Step [2/4], Loss: 0.6834, Acc : 57.8125%\n",
      "Epoch [104/500], Step [4/4], Loss: 0.6751, Acc : 75.0%\n",
      "Epoch [105/500], Step [2/4], Loss: 0.6891, Acc : 50.0%\n",
      "Epoch [105/500], Step [4/4], Loss: 0.6936, Acc : 50.0%\n",
      "Epoch [106/500], Step [2/4], Loss: 0.6878, Acc : 57.8125%\n",
      "Epoch [106/500], Step [4/4], Loss: 0.6887, Acc : 50.0%\n",
      "Epoch [107/500], Step [2/4], Loss: 0.6851, Acc : 68.75%\n",
      "Epoch [107/500], Step [4/4], Loss: 0.6794, Acc : 75.0%\n",
      "Epoch [108/500], Step [2/4], Loss: 0.6857, Acc : 51.5625%\n",
      "Epoch [108/500], Step [4/4], Loss: 0.7398, Acc : 12.5%\n",
      "Epoch [109/500], Step [2/4], Loss: 0.6955, Acc : 42.1875%\n",
      "Epoch [109/500], Step [4/4], Loss: 0.6930, Acc : 50.0%\n",
      "Epoch [110/500], Step [2/4], Loss: 0.6886, Acc : 54.6875%\n",
      "Epoch [110/500], Step [4/4], Loss: 0.6872, Acc : 50.0%\n",
      "Epoch [111/500], Step [2/4], Loss: 0.6868, Acc : 64.0625%\n",
      "Epoch [111/500], Step [4/4], Loss: 0.6923, Acc : 62.5%\n",
      "Epoch [112/500], Step [2/4], Loss: 0.6868, Acc : 64.0625%\n",
      "Epoch [112/500], Step [4/4], Loss: 0.6779, Acc : 62.5%\n",
      "Epoch [113/500], Step [2/4], Loss: 0.6835, Acc : 68.75%\n",
      "Epoch [113/500], Step [4/4], Loss: 0.6999, Acc : 37.5%\n",
      "Epoch [114/500], Step [2/4], Loss: 0.6831, Acc : 67.1875%\n",
      "Epoch [114/500], Step [4/4], Loss: 0.6882, Acc : 75.0%\n",
      "Epoch [115/500], Step [2/4], Loss: 0.6841, Acc : 64.0625%\n",
      "Epoch [115/500], Step [4/4], Loss: 0.6843, Acc : 75.0%\n",
      "Epoch [116/500], Step [2/4], Loss: 0.6872, Acc : 60.9375%\n",
      "Epoch [116/500], Step [4/4], Loss: 0.6810, Acc : 75.0%\n",
      "Epoch [117/500], Step [2/4], Loss: 0.6845, Acc : 65.625%\n",
      "Epoch [117/500], Step [4/4], Loss: 0.6785, Acc : 75.0%\n",
      "Epoch [118/500], Step [2/4], Loss: 0.6849, Acc : 62.5%\n",
      "Epoch [118/500], Step [4/4], Loss: 0.6905, Acc : 37.5%\n",
      "Epoch [119/500], Step [2/4], Loss: 0.6877, Acc : 54.6875%\n",
      "Epoch [119/500], Step [4/4], Loss: 0.7000, Acc : 50.0%\n",
      "Epoch [120/500], Step [2/4], Loss: 0.6833, Acc : 62.5%\n",
      "Epoch [120/500], Step [4/4], Loss: 0.6868, Acc : 50.0%\n",
      "Epoch [121/500], Step [2/4], Loss: 0.6820, Acc : 59.375%\n",
      "Epoch [121/500], Step [4/4], Loss: 0.6810, Acc : 62.5%\n",
      "Epoch [122/500], Step [2/4], Loss: 0.6903, Acc : 53.125%\n",
      "Epoch [122/500], Step [4/4], Loss: 0.6797, Acc : 75.0%\n",
      "Epoch [123/500], Step [2/4], Loss: 0.6905, Acc : 48.4375%\n",
      "Epoch [123/500], Step [4/4], Loss: 0.6999, Acc : 50.0%\n",
      "Epoch [124/500], Step [2/4], Loss: 0.6831, Acc : 62.5%\n",
      "Epoch [124/500], Step [4/4], Loss: 0.6920, Acc : 37.5%\n",
      "Epoch [125/500], Step [2/4], Loss: 0.6811, Acc : 70.3125%\n",
      "Epoch [125/500], Step [4/4], Loss: 0.7040, Acc : 25.0%\n",
      "Epoch [126/500], Step [2/4], Loss: 0.6859, Acc : 59.375%\n",
      "Epoch [126/500], Step [4/4], Loss: 0.6873, Acc : 50.0%\n",
      "Epoch [127/500], Step [2/4], Loss: 0.6863, Acc : 64.0625%\n",
      "Epoch [127/500], Step [4/4], Loss: 0.6819, Acc : 62.5%\n",
      "Epoch [128/500], Step [2/4], Loss: 0.6839, Acc : 64.0625%\n",
      "Epoch [128/500], Step [4/4], Loss: 0.6990, Acc : 50.0%\n",
      "Epoch [129/500], Step [2/4], Loss: 0.6806, Acc : 67.1875%\n",
      "Epoch [129/500], Step [4/4], Loss: 0.6715, Acc : 62.5%\n",
      "Epoch [130/500], Step [2/4], Loss: 0.6800, Acc : 62.5%\n",
      "Epoch [130/500], Step [4/4], Loss: 0.6957, Acc : 37.5%\n",
      "Epoch [131/500], Step [2/4], Loss: 0.6859, Acc : 57.8125%\n",
      "Epoch [131/500], Step [4/4], Loss: 0.6920, Acc : 50.0%\n",
      "Epoch [132/500], Step [2/4], Loss: 0.6797, Acc : 64.0625%\n",
      "Epoch [132/500], Step [4/4], Loss: 0.6833, Acc : 50.0%\n",
      "Epoch [133/500], Step [2/4], Loss: 0.6839, Acc : 59.375%\n",
      "Epoch [133/500], Step [4/4], Loss: 0.6717, Acc : 87.5%\n",
      "Epoch [134/500], Step [2/4], Loss: 0.6860, Acc : 60.9375%\n",
      "Epoch [134/500], Step [4/4], Loss: 0.6727, Acc : 87.5%\n",
      "Epoch [135/500], Step [2/4], Loss: 0.6836, Acc : 59.375%\n",
      "Epoch [135/500], Step [4/4], Loss: 0.6778, Acc : 75.0%\n",
      "Epoch [136/500], Step [2/4], Loss: 0.6822, Acc : 73.4375%\n",
      "Epoch [136/500], Step [4/4], Loss: 0.6766, Acc : 50.0%\n",
      "Epoch [137/500], Step [2/4], Loss: 0.6782, Acc : 73.4375%\n",
      "Epoch [137/500], Step [4/4], Loss: 0.6779, Acc : 75.0%\n",
      "Epoch [138/500], Step [2/4], Loss: 0.6809, Acc : 67.1875%\n",
      "Epoch [138/500], Step [4/4], Loss: 0.6753, Acc : 50.0%\n",
      "Epoch [139/500], Step [2/4], Loss: 0.6803, Acc : 57.8125%\n",
      "Epoch [139/500], Step [4/4], Loss: 0.6757, Acc : 50.0%\n",
      "Epoch [140/500], Step [2/4], Loss: 0.6927, Acc : 53.125%\n",
      "Epoch [140/500], Step [4/4], Loss: 0.6900, Acc : 62.5%\n",
      "Epoch [141/500], Step [2/4], Loss: 0.6889, Acc : 59.375%\n",
      "Epoch [141/500], Step [4/4], Loss: 0.6861, Acc : 62.5%\n",
      "Epoch [142/500], Step [2/4], Loss: 0.6883, Acc : 53.125%\n",
      "Epoch [142/500], Step [4/4], Loss: 0.6796, Acc : 62.5%\n",
      "Epoch [143/500], Step [2/4], Loss: 0.6896, Acc : 50.0%\n",
      "Epoch [143/500], Step [4/4], Loss: 0.6706, Acc : 87.5%\n",
      "Epoch [144/500], Step [2/4], Loss: 0.6913, Acc : 46.875%\n",
      "Epoch [144/500], Step [4/4], Loss: 0.6917, Acc : 62.5%\n",
      "Epoch [145/500], Step [2/4], Loss: 0.6864, Acc : 56.25%\n",
      "Epoch [145/500], Step [4/4], Loss: 0.6612, Acc : 75.0%\n",
      "Epoch [146/500], Step [2/4], Loss: 0.6873, Acc : 51.5625%\n",
      "Epoch [146/500], Step [4/4], Loss: 0.6620, Acc : 87.5%\n",
      "Epoch [147/500], Step [2/4], Loss: 0.6902, Acc : 54.6875%\n",
      "Epoch [147/500], Step [4/4], Loss: 0.6857, Acc : 50.0%\n",
      "Epoch [148/500], Step [2/4], Loss: 0.6831, Acc : 54.6875%\n",
      "Epoch [148/500], Step [4/4], Loss: 0.6484, Acc : 62.5%\n",
      "Epoch [149/500], Step [2/4], Loss: 0.6763, Acc : 56.25%\n",
      "Epoch [149/500], Step [4/4], Loss: 0.7141, Acc : 50.0%\n",
      "Epoch [150/500], Step [2/4], Loss: 0.6960, Acc : 48.4375%\n",
      "Epoch [150/500], Step [4/4], Loss: 0.6728, Acc : 75.0%\n",
      "Epoch [151/500], Step [2/4], Loss: 0.6826, Acc : 60.9375%\n",
      "Epoch [151/500], Step [4/4], Loss: 0.6597, Acc : 75.0%\n",
      "Epoch [152/500], Step [2/4], Loss: 0.6833, Acc : 62.5%\n",
      "Epoch [152/500], Step [4/4], Loss: 0.6720, Acc : 62.5%\n",
      "Epoch [153/500], Step [2/4], Loss: 0.6782, Acc : 57.8125%\n",
      "Epoch [153/500], Step [4/4], Loss: 0.6858, Acc : 62.5%\n",
      "Epoch [154/500], Step [2/4], Loss: 0.6803, Acc : 56.25%\n",
      "Epoch [154/500], Step [4/4], Loss: 0.6882, Acc : 62.5%\n",
      "Epoch [155/500], Step [2/4], Loss: 0.6772, Acc : 64.0625%\n",
      "Epoch [155/500], Step [4/4], Loss: 0.6851, Acc : 37.5%\n",
      "Epoch [156/500], Step [2/4], Loss: 0.6844, Acc : 56.25%\n",
      "Epoch [156/500], Step [4/4], Loss: 0.6760, Acc : 75.0%\n",
      "Epoch [157/500], Step [2/4], Loss: 0.6795, Acc : 73.4375%\n",
      "Epoch [157/500], Step [4/4], Loss: 0.6877, Acc : 37.5%\n",
      "Epoch [158/500], Step [2/4], Loss: 0.6844, Acc : 54.6875%\n",
      "Epoch [158/500], Step [4/4], Loss: 0.6907, Acc : 62.5%\n",
      "Epoch [159/500], Step [2/4], Loss: 0.6852, Acc : 54.6875%\n",
      "Epoch [159/500], Step [4/4], Loss: 0.6672, Acc : 87.5%\n",
      "Epoch [160/500], Step [2/4], Loss: 0.6777, Acc : 64.0625%\n",
      "Epoch [160/500], Step [4/4], Loss: 0.6757, Acc : 75.0%\n",
      "Epoch [161/500], Step [2/4], Loss: 0.6755, Acc : 67.1875%\n",
      "Epoch [161/500], Step [4/4], Loss: 0.6989, Acc : 37.5%\n",
      "Epoch [162/500], Step [2/4], Loss: 0.6724, Acc : 70.3125%\n",
      "Epoch [162/500], Step [4/4], Loss: 0.6922, Acc : 62.5%\n",
      "Epoch [163/500], Step [2/4], Loss: 0.6847, Acc : 59.375%\n",
      "Epoch [163/500], Step [4/4], Loss: 0.7081, Acc : 50.0%\n",
      "Epoch [164/500], Step [2/4], Loss: 0.6895, Acc : 54.6875%\n",
      "Epoch [164/500], Step [4/4], Loss: 0.6794, Acc : 75.0%\n",
      "Epoch [165/500], Step [2/4], Loss: 0.6766, Acc : 64.0625%\n",
      "Epoch [165/500], Step [4/4], Loss: 0.6775, Acc : 50.0%\n",
      "Epoch [166/500], Step [2/4], Loss: 0.6775, Acc : 65.625%\n",
      "Epoch [166/500], Step [4/4], Loss: 0.6635, Acc : 75.0%\n",
      "Epoch [167/500], Step [2/4], Loss: 0.6761, Acc : 62.5%\n",
      "Epoch [167/500], Step [4/4], Loss: 0.6781, Acc : 87.5%\n",
      "Epoch [168/500], Step [2/4], Loss: 0.6793, Acc : 59.375%\n",
      "Epoch [168/500], Step [4/4], Loss: 0.6757, Acc : 50.0%\n",
      "Epoch [169/500], Step [2/4], Loss: 0.6776, Acc : 68.75%\n",
      "Epoch [169/500], Step [4/4], Loss: 0.7025, Acc : 37.5%\n",
      "Epoch [170/500], Step [2/4], Loss: 0.6852, Acc : 64.0625%\n",
      "Epoch [170/500], Step [4/4], Loss: 0.6760, Acc : 62.5%\n",
      "Epoch [171/500], Step [2/4], Loss: 0.6760, Acc : 62.5%\n",
      "Epoch [171/500], Step [4/4], Loss: 0.6694, Acc : 87.5%\n",
      "Epoch [172/500], Step [2/4], Loss: 0.6874, Acc : 56.25%\n",
      "Epoch [172/500], Step [4/4], Loss: 0.6844, Acc : 62.5%\n",
      "Epoch [173/500], Step [2/4], Loss: 0.6792, Acc : 64.0625%\n",
      "Epoch [173/500], Step [4/4], Loss: 0.6716, Acc : 62.5%\n",
      "Epoch [174/500], Step [2/4], Loss: 0.6729, Acc : 71.875%\n",
      "Epoch [174/500], Step [4/4], Loss: 0.6845, Acc : 75.0%\n",
      "Epoch [175/500], Step [2/4], Loss: 0.6767, Acc : 59.375%\n",
      "Epoch [175/500], Step [4/4], Loss: 0.6795, Acc : 62.5%\n",
      "Epoch [176/500], Step [2/4], Loss: 0.6836, Acc : 56.25%\n",
      "Epoch [176/500], Step [4/4], Loss: 0.6668, Acc : 75.0%\n",
      "Epoch [177/500], Step [2/4], Loss: 0.6762, Acc : 68.75%\n",
      "Epoch [177/500], Step [4/4], Loss: 0.6565, Acc : 75.0%\n",
      "Epoch [178/500], Step [2/4], Loss: 0.6827, Acc : 57.8125%\n",
      "Epoch [178/500], Step [4/4], Loss: 0.7090, Acc : 62.5%\n",
      "Epoch [179/500], Step [2/4], Loss: 0.6830, Acc : 60.9375%\n",
      "Epoch [179/500], Step [4/4], Loss: 0.6523, Acc : 62.5%\n",
      "Epoch [180/500], Step [2/4], Loss: 0.6770, Acc : 57.8125%\n",
      "Epoch [180/500], Step [4/4], Loss: 0.7320, Acc : 25.0%\n",
      "Epoch [181/500], Step [2/4], Loss: 0.6833, Acc : 51.5625%\n",
      "Epoch [181/500], Step [4/4], Loss: 0.6579, Acc : 87.5%\n",
      "Epoch [182/500], Step [2/4], Loss: 0.6730, Acc : 65.625%\n",
      "Epoch [182/500], Step [4/4], Loss: 0.6813, Acc : 62.5%\n",
      "Epoch [183/500], Step [2/4], Loss: 0.6753, Acc : 64.0625%\n",
      "Epoch [183/500], Step [4/4], Loss: 0.6794, Acc : 62.5%\n",
      "Epoch [184/500], Step [2/4], Loss: 0.6719, Acc : 64.0625%\n",
      "Epoch [184/500], Step [4/4], Loss: 0.6830, Acc : 62.5%\n",
      "Epoch [185/500], Step [2/4], Loss: 0.6795, Acc : 62.5%\n",
      "Epoch [185/500], Step [4/4], Loss: 0.6805, Acc : 62.5%\n",
      "Epoch [186/500], Step [2/4], Loss: 0.6790, Acc : 62.5%\n",
      "Epoch [186/500], Step [4/4], Loss: 0.6819, Acc : 62.5%\n",
      "Epoch [187/500], Step [2/4], Loss: 0.6869, Acc : 54.6875%\n",
      "Epoch [187/500], Step [4/4], Loss: 0.6848, Acc : 75.0%\n",
      "Epoch [188/500], Step [2/4], Loss: 0.6794, Acc : 62.5%\n",
      "Epoch [188/500], Step [4/4], Loss: 0.6353, Acc : 75.0%\n",
      "Epoch [189/500], Step [2/4], Loss: 0.6764, Acc : 59.375%\n",
      "Epoch [189/500], Step [4/4], Loss: 0.6830, Acc : 62.5%\n",
      "Epoch [190/500], Step [2/4], Loss: 0.6677, Acc : 70.3125%\n",
      "Epoch [190/500], Step [4/4], Loss: 0.6685, Acc : 87.5%\n",
      "Epoch [191/500], Step [2/4], Loss: 0.6763, Acc : 62.5%\n",
      "Epoch [191/500], Step [4/4], Loss: 0.6828, Acc : 62.5%\n",
      "Epoch [192/500], Step [2/4], Loss: 0.6699, Acc : 60.9375%\n",
      "Epoch [192/500], Step [4/4], Loss: 0.6783, Acc : 62.5%\n",
      "Epoch [193/500], Step [2/4], Loss: 0.6804, Acc : 57.8125%\n",
      "Epoch [193/500], Step [4/4], Loss: 0.6805, Acc : 62.5%\n",
      "Epoch [194/500], Step [2/4], Loss: 0.6723, Acc : 60.9375%\n",
      "Epoch [194/500], Step [4/4], Loss: 0.6849, Acc : 62.5%\n",
      "Epoch [195/500], Step [2/4], Loss: 0.6795, Acc : 53.125%\n",
      "Epoch [195/500], Step [4/4], Loss: 0.7004, Acc : 50.0%\n",
      "Epoch [196/500], Step [2/4], Loss: 0.6701, Acc : 60.9375%\n",
      "Epoch [196/500], Step [4/4], Loss: 0.6717, Acc : 75.0%\n",
      "Epoch [197/500], Step [2/4], Loss: 0.6817, Acc : 59.375%\n",
      "Epoch [197/500], Step [4/4], Loss: 0.6710, Acc : 62.5%\n",
      "Epoch [198/500], Step [2/4], Loss: 0.6703, Acc : 67.1875%\n",
      "Epoch [198/500], Step [4/4], Loss: 0.7332, Acc : 37.5%\n",
      "Epoch [199/500], Step [2/4], Loss: 0.6649, Acc : 75.0%\n",
      "Epoch [199/500], Step [4/4], Loss: 0.6752, Acc : 75.0%\n",
      "Epoch [200/500], Step [2/4], Loss: 0.6809, Acc : 56.25%\n",
      "Epoch [200/500], Step [4/4], Loss: 0.6950, Acc : 50.0%\n",
      "Epoch [201/500], Step [2/4], Loss: 0.6699, Acc : 71.875%\n",
      "Epoch [201/500], Step [4/4], Loss: 0.6572, Acc : 87.5%\n",
      "Epoch [202/500], Step [2/4], Loss: 0.6741, Acc : 62.5%\n",
      "Epoch [202/500], Step [4/4], Loss: 0.6608, Acc : 75.0%\n",
      "Epoch [203/500], Step [2/4], Loss: 0.6717, Acc : 57.8125%\n",
      "Epoch [203/500], Step [4/4], Loss: 0.6823, Acc : 75.0%\n",
      "Epoch [204/500], Step [2/4], Loss: 0.6669, Acc : 67.1875%\n",
      "Epoch [204/500], Step [4/4], Loss: 0.6543, Acc : 62.5%\n",
      "Epoch [205/500], Step [2/4], Loss: 0.6696, Acc : 59.375%\n",
      "Epoch [205/500], Step [4/4], Loss: 0.6419, Acc : 75.0%\n",
      "Epoch [206/500], Step [2/4], Loss: 0.6732, Acc : 56.25%\n",
      "Epoch [206/500], Step [4/4], Loss: 0.6371, Acc : 75.0%\n",
      "Epoch [207/500], Step [2/4], Loss: 0.6733, Acc : 56.25%\n",
      "Epoch [207/500], Step [4/4], Loss: 0.6946, Acc : 37.5%\n",
      "Epoch [208/500], Step [2/4], Loss: 0.6667, Acc : 64.0625%\n",
      "Epoch [208/500], Step [4/4], Loss: 0.7186, Acc : 37.5%\n",
      "Epoch [209/500], Step [2/4], Loss: 0.6774, Acc : 57.8125%\n",
      "Epoch [209/500], Step [4/4], Loss: 0.6678, Acc : 62.5%\n",
      "Epoch [210/500], Step [2/4], Loss: 0.6669, Acc : 71.875%\n",
      "Epoch [210/500], Step [4/4], Loss: 0.6841, Acc : 62.5%\n",
      "Epoch [211/500], Step [2/4], Loss: 0.6726, Acc : 65.625%\n",
      "Epoch [211/500], Step [4/4], Loss: 0.7173, Acc : 50.0%\n",
      "Epoch [212/500], Step [2/4], Loss: 0.6806, Acc : 54.6875%\n",
      "Epoch [212/500], Step [4/4], Loss: 0.6542, Acc : 75.0%\n",
      "Epoch [213/500], Step [2/4], Loss: 0.6626, Acc : 76.5625%\n",
      "Epoch [213/500], Step [4/4], Loss: 0.7059, Acc : 50.0%\n",
      "Epoch [214/500], Step [2/4], Loss: 0.6699, Acc : 67.1875%\n",
      "Epoch [214/500], Step [4/4], Loss: 0.6957, Acc : 37.5%\n",
      "Epoch [215/500], Step [2/4], Loss: 0.6756, Acc : 54.6875%\n",
      "Epoch [215/500], Step [4/4], Loss: 0.6525, Acc : 87.5%\n",
      "Epoch [216/500], Step [2/4], Loss: 0.6706, Acc : 62.5%\n",
      "Epoch [216/500], Step [4/4], Loss: 0.6761, Acc : 62.5%\n",
      "Epoch [217/500], Step [2/4], Loss: 0.6715, Acc : 60.9375%\n",
      "Epoch [217/500], Step [4/4], Loss: 0.6975, Acc : 25.0%\n",
      "Epoch [218/500], Step [2/4], Loss: 0.6668, Acc : 65.625%\n",
      "Epoch [218/500], Step [4/4], Loss: 0.6950, Acc : 37.5%\n",
      "Epoch [219/500], Step [2/4], Loss: 0.6668, Acc : 60.9375%\n",
      "Epoch [219/500], Step [4/4], Loss: 0.6980, Acc : 75.0%\n",
      "Epoch [220/500], Step [2/4], Loss: 0.6715, Acc : 60.9375%\n",
      "Epoch [220/500], Step [4/4], Loss: 0.6632, Acc : 75.0%\n",
      "Epoch [221/500], Step [2/4], Loss: 0.6703, Acc : 64.0625%\n",
      "Epoch [221/500], Step [4/4], Loss: 0.6767, Acc : 75.0%\n",
      "Epoch [222/500], Step [2/4], Loss: 0.6659, Acc : 67.1875%\n",
      "Epoch [222/500], Step [4/4], Loss: 0.6706, Acc : 62.5%\n",
      "Epoch [223/500], Step [2/4], Loss: 0.6822, Acc : 57.8125%\n",
      "Epoch [223/500], Step [4/4], Loss: 0.6615, Acc : 75.0%\n",
      "Epoch [224/500], Step [2/4], Loss: 0.6701, Acc : 64.0625%\n",
      "Epoch [224/500], Step [4/4], Loss: 0.6417, Acc : 75.0%\n",
      "Epoch [225/500], Step [2/4], Loss: 0.6729, Acc : 62.5%\n",
      "Epoch [225/500], Step [4/4], Loss: 0.6474, Acc : 75.0%\n",
      "Epoch [226/500], Step [2/4], Loss: 0.6751, Acc : 67.1875%\n",
      "Epoch [226/500], Step [4/4], Loss: 0.6433, Acc : 87.5%\n",
      "Epoch [227/500], Step [2/4], Loss: 0.6802, Acc : 59.375%\n",
      "Epoch [227/500], Step [4/4], Loss: 0.6795, Acc : 50.0%\n",
      "Epoch [228/500], Step [2/4], Loss: 0.6799, Acc : 62.5%\n",
      "Epoch [228/500], Step [4/4], Loss: 0.7088, Acc : 50.0%\n",
      "Epoch [229/500], Step [2/4], Loss: 0.6762, Acc : 64.0625%\n",
      "Epoch [229/500], Step [4/4], Loss: 0.6157, Acc : 87.5%\n",
      "Epoch [230/500], Step [2/4], Loss: 0.6615, Acc : 65.625%\n",
      "Epoch [230/500], Step [4/4], Loss: 0.6705, Acc : 62.5%\n",
      "Epoch [231/500], Step [2/4], Loss: 0.6496, Acc : 67.1875%\n",
      "Epoch [231/500], Step [4/4], Loss: 0.6903, Acc : 62.5%\n",
      "Epoch [232/500], Step [2/4], Loss: 0.6693, Acc : 56.25%\n",
      "Epoch [232/500], Step [4/4], Loss: 0.7586, Acc : 25.0%\n",
      "Epoch [233/500], Step [2/4], Loss: 0.6744, Acc : 57.8125%\n",
      "Epoch [233/500], Step [4/4], Loss: 0.7241, Acc : 25.0%\n",
      "Epoch [234/500], Step [2/4], Loss: 0.6675, Acc : 68.75%\n",
      "Epoch [234/500], Step [4/4], Loss: 0.6547, Acc : 75.0%\n",
      "Epoch [235/500], Step [2/4], Loss: 0.6679, Acc : 67.1875%\n",
      "Epoch [235/500], Step [4/4], Loss: 0.6908, Acc : 75.0%\n",
      "Epoch [236/500], Step [2/4], Loss: 0.6644, Acc : 68.75%\n",
      "Epoch [236/500], Step [4/4], Loss: 0.7130, Acc : 50.0%\n",
      "Epoch [237/500], Step [2/4], Loss: 0.6657, Acc : 57.8125%\n",
      "Epoch [237/500], Step [4/4], Loss: 0.6907, Acc : 62.5%\n",
      "Epoch [238/500], Step [2/4], Loss: 0.6535, Acc : 71.875%\n",
      "Epoch [238/500], Step [4/4], Loss: 0.6648, Acc : 50.0%\n",
      "Epoch [239/500], Step [2/4], Loss: 0.6635, Acc : 67.1875%\n",
      "Epoch [239/500], Step [4/4], Loss: 0.6698, Acc : 75.0%\n",
      "Epoch [240/500], Step [2/4], Loss: 0.6763, Acc : 64.0625%\n",
      "Epoch [240/500], Step [4/4], Loss: 0.7144, Acc : 37.5%\n",
      "Epoch [241/500], Step [2/4], Loss: 0.6978, Acc : 53.125%\n",
      "Epoch [241/500], Step [4/4], Loss: 0.6708, Acc : 75.0%\n",
      "Epoch [242/500], Step [2/4], Loss: 0.6701, Acc : 64.0625%\n",
      "Epoch [242/500], Step [4/4], Loss: 0.6455, Acc : 75.0%\n",
      "Epoch [243/500], Step [2/4], Loss: 0.6663, Acc : 64.0625%\n",
      "Epoch [243/500], Step [4/4], Loss: 0.7139, Acc : 37.5%\n",
      "Epoch [244/500], Step [2/4], Loss: 0.6603, Acc : 71.875%\n",
      "Epoch [244/500], Step [4/4], Loss: 0.6764, Acc : 75.0%\n",
      "Epoch [245/500], Step [2/4], Loss: 0.6698, Acc : 59.375%\n",
      "Epoch [245/500], Step [4/4], Loss: 0.6221, Acc : 100.0%\n",
      "Epoch [246/500], Step [2/4], Loss: 0.6857, Acc : 51.5625%\n",
      "Epoch [246/500], Step [4/4], Loss: 0.6190, Acc : 75.0%\n",
      "Epoch [247/500], Step [2/4], Loss: 0.6525, Acc : 62.5%\n",
      "Epoch [247/500], Step [4/4], Loss: 0.6309, Acc : 100.0%\n",
      "Epoch [248/500], Step [2/4], Loss: 0.6697, Acc : 64.0625%\n",
      "Epoch [248/500], Step [4/4], Loss: 0.6250, Acc : 75.0%\n",
      "Epoch [249/500], Step [2/4], Loss: 0.6694, Acc : 56.25%\n",
      "Epoch [249/500], Step [4/4], Loss: 0.6281, Acc : 75.0%\n",
      "Epoch [250/500], Step [2/4], Loss: 0.6595, Acc : 59.375%\n",
      "Epoch [250/500], Step [4/4], Loss: 0.6445, Acc : 62.5%\n",
      "Epoch [251/500], Step [2/4], Loss: 0.6772, Acc : 56.25%\n",
      "Epoch [251/500], Step [4/4], Loss: 0.6705, Acc : 50.0%\n",
      "Epoch [252/500], Step [2/4], Loss: 0.6689, Acc : 60.9375%\n",
      "Epoch [252/500], Step [4/4], Loss: 0.6022, Acc : 75.0%\n",
      "Epoch [253/500], Step [2/4], Loss: 0.6657, Acc : 62.5%\n",
      "Epoch [253/500], Step [4/4], Loss: 0.6401, Acc : 75.0%\n",
      "Epoch [254/500], Step [2/4], Loss: 0.6712, Acc : 59.375%\n",
      "Epoch [254/500], Step [4/4], Loss: 0.6597, Acc : 75.0%\n",
      "Epoch [255/500], Step [2/4], Loss: 0.6712, Acc : 56.25%\n",
      "Epoch [255/500], Step [4/4], Loss: 0.6449, Acc : 62.5%\n",
      "Epoch [256/500], Step [2/4], Loss: 0.6699, Acc : 67.1875%\n",
      "Epoch [256/500], Step [4/4], Loss: 0.6781, Acc : 62.5%\n",
      "Epoch [257/500], Step [2/4], Loss: 0.6687, Acc : 62.5%\n",
      "Epoch [257/500], Step [4/4], Loss: 0.6470, Acc : 62.5%\n",
      "Epoch [258/500], Step [2/4], Loss: 0.6679, Acc : 65.625%\n",
      "Epoch [258/500], Step [4/4], Loss: 0.6202, Acc : 75.0%\n",
      "Epoch [259/500], Step [2/4], Loss: 0.6473, Acc : 71.875%\n",
      "Epoch [259/500], Step [4/4], Loss: 0.7237, Acc : 37.5%\n",
      "Epoch [260/500], Step [2/4], Loss: 0.6677, Acc : 57.8125%\n",
      "Epoch [260/500], Step [4/4], Loss: 0.6355, Acc : 75.0%\n",
      "Epoch [261/500], Step [2/4], Loss: 0.6802, Acc : 56.25%\n",
      "Epoch [261/500], Step [4/4], Loss: 0.6331, Acc : 75.0%\n",
      "Epoch [262/500], Step [2/4], Loss: 0.6909, Acc : 50.0%\n",
      "Epoch [262/500], Step [4/4], Loss: 0.6983, Acc : 50.0%\n",
      "Epoch [263/500], Step [2/4], Loss: 0.6991, Acc : 46.875%\n",
      "Epoch [263/500], Step [4/4], Loss: 0.6218, Acc : 75.0%\n",
      "Epoch [264/500], Step [2/4], Loss: 0.6540, Acc : 68.75%\n",
      "Epoch [264/500], Step [4/4], Loss: 0.6697, Acc : 50.0%\n",
      "Epoch [265/500], Step [2/4], Loss: 0.6663, Acc : 64.0625%\n",
      "Epoch [265/500], Step [4/4], Loss: 0.6360, Acc : 50.0%\n",
      "Epoch [266/500], Step [2/4], Loss: 0.6585, Acc : 67.1875%\n",
      "Epoch [266/500], Step [4/4], Loss: 0.6662, Acc : 62.5%\n",
      "Epoch [267/500], Step [2/4], Loss: 0.6831, Acc : 57.8125%\n",
      "Epoch [267/500], Step [4/4], Loss: 0.7050, Acc : 62.5%\n",
      "Epoch [268/500], Step [2/4], Loss: 0.6729, Acc : 65.625%\n",
      "Epoch [268/500], Step [4/4], Loss: 0.6951, Acc : 25.0%\n",
      "Epoch [269/500], Step [2/4], Loss: 0.6640, Acc : 67.1875%\n",
      "Epoch [269/500], Step [4/4], Loss: 0.6391, Acc : 62.5%\n",
      "Epoch [270/500], Step [2/4], Loss: 0.6788, Acc : 57.8125%\n",
      "Epoch [270/500], Step [4/4], Loss: 0.6320, Acc : 100.0%\n",
      "Epoch [271/500], Step [2/4], Loss: 0.6684, Acc : 54.6875%\n",
      "Epoch [271/500], Step [4/4], Loss: 0.7217, Acc : 25.0%\n",
      "Epoch [272/500], Step [2/4], Loss: 0.6756, Acc : 51.5625%\n",
      "Epoch [272/500], Step [4/4], Loss: 0.6615, Acc : 75.0%\n",
      "Epoch [273/500], Step [2/4], Loss: 0.6651, Acc : 59.375%\n",
      "Epoch [273/500], Step [4/4], Loss: 0.6778, Acc : 50.0%\n",
      "Epoch [274/500], Step [2/4], Loss: 0.6651, Acc : 59.375%\n",
      "Epoch [274/500], Step [4/4], Loss: 0.7033, Acc : 50.0%\n",
      "Epoch [275/500], Step [2/4], Loss: 0.6728, Acc : 65.625%\n",
      "Epoch [275/500], Step [4/4], Loss: 0.6812, Acc : 50.0%\n",
      "Epoch [276/500], Step [2/4], Loss: 0.6605, Acc : 67.1875%\n",
      "Epoch [276/500], Step [4/4], Loss: 0.6155, Acc : 87.5%\n",
      "Epoch [277/500], Step [2/4], Loss: 0.6765, Acc : 62.5%\n",
      "Epoch [277/500], Step [4/4], Loss: 0.7050, Acc : 37.5%\n",
      "Epoch [278/500], Step [2/4], Loss: 0.6644, Acc : 67.1875%\n",
      "Epoch [278/500], Step [4/4], Loss: 0.6690, Acc : 62.5%\n",
      "Epoch [279/500], Step [2/4], Loss: 0.6739, Acc : 57.8125%\n",
      "Epoch [279/500], Step [4/4], Loss: 0.6417, Acc : 87.5%\n",
      "Epoch [280/500], Step [2/4], Loss: 0.6720, Acc : 59.375%\n",
      "Epoch [280/500], Step [4/4], Loss: 0.6607, Acc : 75.0%\n",
      "Epoch [281/500], Step [2/4], Loss: 0.6680, Acc : 60.9375%\n",
      "Epoch [281/500], Step [4/4], Loss: 0.6376, Acc : 62.5%\n",
      "Epoch [282/500], Step [2/4], Loss: 0.6553, Acc : 68.75%\n",
      "Epoch [282/500], Step [4/4], Loss: 0.6937, Acc : 37.5%\n",
      "Epoch [283/500], Step [2/4], Loss: 0.6679, Acc : 59.375%\n",
      "Epoch [283/500], Step [4/4], Loss: 0.6558, Acc : 75.0%\n",
      "Epoch [284/500], Step [2/4], Loss: 0.6706, Acc : 59.375%\n",
      "Epoch [284/500], Step [4/4], Loss: 0.6789, Acc : 50.0%\n",
      "Epoch [285/500], Step [2/4], Loss: 0.6600, Acc : 62.5%\n",
      "Epoch [285/500], Step [4/4], Loss: 0.6860, Acc : 62.5%\n",
      "Epoch [286/500], Step [2/4], Loss: 0.6562, Acc : 67.1875%\n",
      "Epoch [286/500], Step [4/4], Loss: 0.6535, Acc : 75.0%\n",
      "Epoch [287/500], Step [2/4], Loss: 0.6644, Acc : 68.75%\n",
      "Epoch [287/500], Step [4/4], Loss: 0.6362, Acc : 75.0%\n",
      "Epoch [288/500], Step [2/4], Loss: 0.6773, Acc : 57.8125%\n",
      "Epoch [288/500], Step [4/4], Loss: 0.6445, Acc : 87.5%\n",
      "Epoch [289/500], Step [2/4], Loss: 0.6746, Acc : 46.875%\n",
      "Epoch [289/500], Step [4/4], Loss: 0.6408, Acc : 75.0%\n",
      "Epoch [290/500], Step [2/4], Loss: 0.6742, Acc : 59.375%\n",
      "Epoch [290/500], Step [4/4], Loss: 0.7475, Acc : 25.0%\n",
      "Epoch [291/500], Step [2/4], Loss: 0.6475, Acc : 67.1875%\n",
      "Epoch [291/500], Step [4/4], Loss: 0.7132, Acc : 50.0%\n",
      "Epoch [292/500], Step [2/4], Loss: 0.6757, Acc : 56.25%\n",
      "Epoch [292/500], Step [4/4], Loss: 0.6076, Acc : 87.5%\n",
      "Epoch [293/500], Step [2/4], Loss: 0.6456, Acc : 70.3125%\n",
      "Epoch [293/500], Step [4/4], Loss: 0.6402, Acc : 75.0%\n",
      "Epoch [294/500], Step [2/4], Loss: 0.6680, Acc : 62.5%\n",
      "Epoch [294/500], Step [4/4], Loss: 0.7151, Acc : 50.0%\n",
      "Epoch [295/500], Step [2/4], Loss: 0.6699, Acc : 57.8125%\n",
      "Epoch [295/500], Step [4/4], Loss: 0.6597, Acc : 62.5%\n",
      "Epoch [296/500], Step [2/4], Loss: 0.6667, Acc : 59.375%\n",
      "Epoch [296/500], Step [4/4], Loss: 0.6028, Acc : 87.5%\n",
      "Epoch [297/500], Step [2/4], Loss: 0.6682, Acc : 60.9375%\n",
      "Epoch [297/500], Step [4/4], Loss: 0.6337, Acc : 75.0%\n",
      "Epoch [298/500], Step [2/4], Loss: 0.6594, Acc : 62.5%\n",
      "Epoch [298/500], Step [4/4], Loss: 0.6590, Acc : 50.0%\n",
      "Epoch [299/500], Step [2/4], Loss: 0.6578, Acc : 67.1875%\n",
      "Epoch [299/500], Step [4/4], Loss: 0.6691, Acc : 62.5%\n",
      "Epoch [300/500], Step [2/4], Loss: 0.6610, Acc : 65.625%\n",
      "Epoch [300/500], Step [4/4], Loss: 0.6571, Acc : 62.5%\n",
      "Epoch [301/500], Step [2/4], Loss: 0.6650, Acc : 60.9375%\n",
      "Epoch [301/500], Step [4/4], Loss: 0.6755, Acc : 37.5%\n",
      "Epoch [302/500], Step [2/4], Loss: 0.6554, Acc : 60.9375%\n",
      "Epoch [302/500], Step [4/4], Loss: 0.7071, Acc : 37.5%\n",
      "Epoch [303/500], Step [2/4], Loss: 0.6590, Acc : 64.0625%\n",
      "Epoch [303/500], Step [4/4], Loss: 0.6875, Acc : 62.5%\n",
      "Epoch [304/500], Step [2/4], Loss: 0.6618, Acc : 57.8125%\n",
      "Epoch [304/500], Step [4/4], Loss: 0.5915, Acc : 87.5%\n",
      "Epoch [305/500], Step [2/4], Loss: 0.6698, Acc : 59.375%\n",
      "Epoch [305/500], Step [4/4], Loss: 0.7047, Acc : 37.5%\n",
      "Epoch [306/500], Step [2/4], Loss: 0.6584, Acc : 60.9375%\n",
      "Epoch [306/500], Step [4/4], Loss: 0.6854, Acc : 62.5%\n",
      "Epoch [307/500], Step [2/4], Loss: 0.6704, Acc : 64.0625%\n",
      "Epoch [307/500], Step [4/4], Loss: 0.6463, Acc : 62.5%\n",
      "Epoch [308/500], Step [2/4], Loss: 0.6454, Acc : 70.3125%\n",
      "Epoch [308/500], Step [4/4], Loss: 0.6492, Acc : 75.0%\n",
      "Epoch [309/500], Step [2/4], Loss: 0.6346, Acc : 76.5625%\n",
      "Epoch [309/500], Step [4/4], Loss: 0.6498, Acc : 50.0%\n",
      "Epoch [310/500], Step [2/4], Loss: 0.6811, Acc : 59.375%\n",
      "Epoch [310/500], Step [4/4], Loss: 0.6448, Acc : 75.0%\n",
      "Epoch [311/500], Step [2/4], Loss: 0.6556, Acc : 64.0625%\n",
      "Epoch [311/500], Step [4/4], Loss: 0.6593, Acc : 75.0%\n",
      "Epoch [312/500], Step [2/4], Loss: 0.6705, Acc : 62.5%\n",
      "Epoch [312/500], Step [4/4], Loss: 0.6336, Acc : 62.5%\n",
      "Epoch [313/500], Step [2/4], Loss: 0.6487, Acc : 64.0625%\n",
      "Epoch [313/500], Step [4/4], Loss: 0.7301, Acc : 37.5%\n",
      "Epoch [314/500], Step [2/4], Loss: 0.6723, Acc : 51.5625%\n",
      "Epoch [314/500], Step [4/4], Loss: 0.6660, Acc : 75.0%\n",
      "Epoch [315/500], Step [2/4], Loss: 0.6609, Acc : 67.1875%\n",
      "Epoch [315/500], Step [4/4], Loss: 0.6226, Acc : 62.5%\n",
      "Epoch [316/500], Step [2/4], Loss: 0.6582, Acc : 64.0625%\n",
      "Epoch [316/500], Step [4/4], Loss: 0.6982, Acc : 37.5%\n",
      "Epoch [317/500], Step [2/4], Loss: 0.6528, Acc : 71.875%\n",
      "Epoch [317/500], Step [4/4], Loss: 0.6949, Acc : 37.5%\n",
      "Epoch [318/500], Step [2/4], Loss: 0.6671, Acc : 57.8125%\n",
      "Epoch [318/500], Step [4/4], Loss: 0.6682, Acc : 50.0%\n",
      "Epoch [319/500], Step [2/4], Loss: 0.6746, Acc : 57.8125%\n",
      "Epoch [319/500], Step [4/4], Loss: 0.6484, Acc : 87.5%\n",
      "Epoch [320/500], Step [2/4], Loss: 0.6640, Acc : 60.9375%\n",
      "Epoch [320/500], Step [4/4], Loss: 0.6632, Acc : 50.0%\n",
      "Epoch [321/500], Step [2/4], Loss: 0.6814, Acc : 59.375%\n",
      "Epoch [321/500], Step [4/4], Loss: 0.6431, Acc : 62.5%\n",
      "Epoch [322/500], Step [2/4], Loss: 0.6560, Acc : 70.3125%\n",
      "Epoch [322/500], Step [4/4], Loss: 0.7079, Acc : 37.5%\n",
      "Epoch [323/500], Step [2/4], Loss: 0.6793, Acc : 59.375%\n",
      "Epoch [323/500], Step [4/4], Loss: 0.6733, Acc : 62.5%\n",
      "Epoch [324/500], Step [2/4], Loss: 0.6606, Acc : 65.625%\n",
      "Epoch [324/500], Step [4/4], Loss: 0.7144, Acc : 62.5%\n",
      "Epoch [325/500], Step [2/4], Loss: 0.6598, Acc : 64.0625%\n",
      "Epoch [325/500], Step [4/4], Loss: 0.6205, Acc : 87.5%\n",
      "Epoch [326/500], Step [2/4], Loss: 0.6663, Acc : 59.375%\n",
      "Epoch [326/500], Step [4/4], Loss: 0.6502, Acc : 62.5%\n",
      "Epoch [327/500], Step [2/4], Loss: 0.6500, Acc : 64.0625%\n",
      "Epoch [327/500], Step [4/4], Loss: 0.6332, Acc : 62.5%\n",
      "Epoch [328/500], Step [2/4], Loss: 0.6581, Acc : 59.375%\n",
      "Epoch [328/500], Step [4/4], Loss: 0.6065, Acc : 75.0%\n",
      "Epoch [329/500], Step [2/4], Loss: 0.6588, Acc : 62.5%\n",
      "Epoch [329/500], Step [4/4], Loss: 0.6668, Acc : 75.0%\n",
      "Epoch [330/500], Step [2/4], Loss: 0.6661, Acc : 59.375%\n",
      "Epoch [330/500], Step [4/4], Loss: 0.6717, Acc : 50.0%\n",
      "Epoch [331/500], Step [2/4], Loss: 0.6767, Acc : 56.25%\n",
      "Epoch [331/500], Step [4/4], Loss: 0.6465, Acc : 75.0%\n",
      "Epoch [332/500], Step [2/4], Loss: 0.6359, Acc : 70.3125%\n",
      "Epoch [332/500], Step [4/4], Loss: 0.6700, Acc : 62.5%\n",
      "Epoch [333/500], Step [2/4], Loss: 0.6717, Acc : 57.8125%\n",
      "Epoch [333/500], Step [4/4], Loss: 0.6665, Acc : 37.5%\n",
      "Epoch [334/500], Step [2/4], Loss: 0.6694, Acc : 60.9375%\n",
      "Epoch [334/500], Step [4/4], Loss: 0.6615, Acc : 62.5%\n",
      "Epoch [335/500], Step [2/4], Loss: 0.6366, Acc : 71.875%\n",
      "Epoch [335/500], Step [4/4], Loss: 0.6662, Acc : 50.0%\n",
      "Epoch [336/500], Step [2/4], Loss: 0.6480, Acc : 59.375%\n",
      "Epoch [336/500], Step [4/4], Loss: 0.6455, Acc : 75.0%\n",
      "Epoch [337/500], Step [2/4], Loss: 0.6655, Acc : 64.0625%\n",
      "Epoch [337/500], Step [4/4], Loss: 0.5833, Acc : 75.0%\n",
      "Epoch [338/500], Step [2/4], Loss: 0.6750, Acc : 59.375%\n",
      "Epoch [338/500], Step [4/4], Loss: 0.5901, Acc : 87.5%\n",
      "Epoch [339/500], Step [2/4], Loss: 0.6700, Acc : 51.5625%\n",
      "Epoch [339/500], Step [4/4], Loss: 0.6678, Acc : 62.5%\n",
      "Epoch [340/500], Step [2/4], Loss: 0.6651, Acc : 62.5%\n",
      "Epoch [340/500], Step [4/4], Loss: 0.7017, Acc : 50.0%\n",
      "Epoch [341/500], Step [2/4], Loss: 0.6829, Acc : 48.4375%\n",
      "Epoch [341/500], Step [4/4], Loss: 0.6139, Acc : 75.0%\n",
      "Epoch [342/500], Step [2/4], Loss: 0.6569, Acc : 64.0625%\n",
      "Epoch [342/500], Step [4/4], Loss: 0.5813, Acc : 75.0%\n",
      "Epoch [343/500], Step [2/4], Loss: 0.6547, Acc : 60.9375%\n",
      "Epoch [343/500], Step [4/4], Loss: 0.6499, Acc : 62.5%\n",
      "Epoch [344/500], Step [2/4], Loss: 0.6841, Acc : 54.6875%\n",
      "Epoch [344/500], Step [4/4], Loss: 0.6550, Acc : 62.5%\n",
      "Epoch [345/500], Step [2/4], Loss: 0.6738, Acc : 51.5625%\n",
      "Epoch [345/500], Step [4/4], Loss: 0.6555, Acc : 50.0%\n",
      "Epoch [346/500], Step [2/4], Loss: 0.6518, Acc : 65.625%\n",
      "Epoch [346/500], Step [4/4], Loss: 0.6444, Acc : 62.5%\n",
      "Epoch [347/500], Step [2/4], Loss: 0.6491, Acc : 65.625%\n",
      "Epoch [347/500], Step [4/4], Loss: 0.6525, Acc : 62.5%\n",
      "Epoch [348/500], Step [2/4], Loss: 0.6524, Acc : 62.5%\n",
      "Epoch [348/500], Step [4/4], Loss: 0.6675, Acc : 62.5%\n",
      "Epoch [349/500], Step [2/4], Loss: 0.6411, Acc : 70.3125%\n",
      "Epoch [349/500], Step [4/4], Loss: 0.6554, Acc : 62.5%\n",
      "Epoch [350/500], Step [2/4], Loss: 0.6369, Acc : 70.3125%\n",
      "Epoch [350/500], Step [4/4], Loss: 0.6908, Acc : 50.0%\n",
      "Epoch [351/500], Step [2/4], Loss: 0.6397, Acc : 68.75%\n",
      "Epoch [351/500], Step [4/4], Loss: 0.6240, Acc : 75.0%\n",
      "Epoch [352/500], Step [2/4], Loss: 0.6858, Acc : 54.6875%\n",
      "Epoch [352/500], Step [4/4], Loss: 0.6886, Acc : 37.5%\n",
      "Epoch [353/500], Step [2/4], Loss: 0.6551, Acc : 60.9375%\n",
      "Epoch [353/500], Step [4/4], Loss: 0.6049, Acc : 62.5%\n",
      "Epoch [354/500], Step [2/4], Loss: 0.6509, Acc : 67.1875%\n",
      "Epoch [354/500], Step [4/4], Loss: 0.6594, Acc : 75.0%\n",
      "Epoch [355/500], Step [2/4], Loss: 0.6669, Acc : 57.8125%\n",
      "Epoch [355/500], Step [4/4], Loss: 0.6709, Acc : 62.5%\n",
      "Epoch [356/500], Step [2/4], Loss: 0.6932, Acc : 53.125%\n",
      "Epoch [356/500], Step [4/4], Loss: 0.7529, Acc : 25.0%\n",
      "Epoch [357/500], Step [2/4], Loss: 0.6643, Acc : 53.125%\n",
      "Epoch [357/500], Step [4/4], Loss: 0.7175, Acc : 37.5%\n",
      "Epoch [358/500], Step [2/4], Loss: 0.6588, Acc : 67.1875%\n",
      "Epoch [358/500], Step [4/4], Loss: 0.6069, Acc : 75.0%\n",
      "Epoch [359/500], Step [2/4], Loss: 0.6307, Acc : 71.875%\n",
      "Epoch [359/500], Step [4/4], Loss: 0.6511, Acc : 75.0%\n",
      "Epoch [360/500], Step [2/4], Loss: 0.6656, Acc : 60.9375%\n",
      "Epoch [360/500], Step [4/4], Loss: 0.6955, Acc : 62.5%\n",
      "Epoch [361/500], Step [2/4], Loss: 0.7243, Acc : 46.875%\n",
      "Epoch [361/500], Step [4/4], Loss: 0.6401, Acc : 62.5%\n",
      "Epoch [362/500], Step [2/4], Loss: 0.6612, Acc : 62.5%\n",
      "Epoch [362/500], Step [4/4], Loss: 0.6983, Acc : 50.0%\n",
      "Epoch [363/500], Step [2/4], Loss: 0.6494, Acc : 62.5%\n",
      "Epoch [363/500], Step [4/4], Loss: 0.5771, Acc : 75.0%\n",
      "Epoch [364/500], Step [2/4], Loss: 0.6487, Acc : 60.9375%\n",
      "Epoch [364/500], Step [4/4], Loss: 0.6423, Acc : 62.5%\n",
      "Epoch [365/500], Step [2/4], Loss: 0.6466, Acc : 65.625%\n",
      "Epoch [365/500], Step [4/4], Loss: 0.5991, Acc : 75.0%\n",
      "Epoch [366/500], Step [2/4], Loss: 0.6563, Acc : 62.5%\n",
      "Epoch [366/500], Step [4/4], Loss: 0.6118, Acc : 75.0%\n",
      "Epoch [367/500], Step [2/4], Loss: 0.6540, Acc : 62.5%\n",
      "Epoch [367/500], Step [4/4], Loss: 0.6483, Acc : 50.0%\n",
      "Epoch [368/500], Step [2/4], Loss: 0.6624, Acc : 57.8125%\n",
      "Epoch [368/500], Step [4/4], Loss: 0.6682, Acc : 50.0%\n",
      "Epoch [369/500], Step [2/4], Loss: 0.6530, Acc : 65.625%\n",
      "Epoch [369/500], Step [4/4], Loss: 0.6365, Acc : 75.0%\n",
      "Epoch [370/500], Step [2/4], Loss: 0.6463, Acc : 65.625%\n",
      "Epoch [370/500], Step [4/4], Loss: 0.5555, Acc : 100.0%\n",
      "Epoch [371/500], Step [2/4], Loss: 0.6634, Acc : 57.8125%\n",
      "Epoch [371/500], Step [4/4], Loss: 0.5880, Acc : 87.5%\n",
      "Epoch [372/500], Step [2/4], Loss: 0.6638, Acc : 54.6875%\n",
      "Epoch [372/500], Step [4/4], Loss: 0.6225, Acc : 62.5%\n",
      "Epoch [373/500], Step [2/4], Loss: 0.6285, Acc : 70.3125%\n",
      "Epoch [373/500], Step [4/4], Loss: 0.6218, Acc : 87.5%\n",
      "Epoch [374/500], Step [2/4], Loss: 0.6495, Acc : 67.1875%\n",
      "Epoch [374/500], Step [4/4], Loss: 0.6628, Acc : 62.5%\n",
      "Epoch [375/500], Step [2/4], Loss: 0.6697, Acc : 60.9375%\n",
      "Epoch [375/500], Step [4/4], Loss: 0.6236, Acc : 62.5%\n",
      "Epoch [376/500], Step [2/4], Loss: 0.6295, Acc : 70.3125%\n",
      "Epoch [376/500], Step [4/4], Loss: 0.6958, Acc : 37.5%\n",
      "Epoch [377/500], Step [2/4], Loss: 0.6531, Acc : 60.9375%\n",
      "Epoch [377/500], Step [4/4], Loss: 0.7122, Acc : 50.0%\n",
      "Epoch [378/500], Step [2/4], Loss: 0.6581, Acc : 64.0625%\n",
      "Epoch [378/500], Step [4/4], Loss: 0.6482, Acc : 75.0%\n",
      "Epoch [379/500], Step [2/4], Loss: 0.6274, Acc : 65.625%\n",
      "Epoch [379/500], Step [4/4], Loss: 0.7126, Acc : 37.5%\n",
      "Epoch [380/500], Step [2/4], Loss: 0.6541, Acc : 59.375%\n",
      "Epoch [380/500], Step [4/4], Loss: 0.6818, Acc : 50.0%\n",
      "Epoch [381/500], Step [2/4], Loss: 0.6394, Acc : 68.75%\n",
      "Epoch [381/500], Step [4/4], Loss: 0.7004, Acc : 62.5%\n",
      "Epoch [382/500], Step [2/4], Loss: 0.6335, Acc : 76.5625%\n",
      "Epoch [382/500], Step [4/4], Loss: 0.6053, Acc : 62.5%\n",
      "Epoch [383/500], Step [2/4], Loss: 0.6277, Acc : 73.4375%\n",
      "Epoch [383/500], Step [4/4], Loss: 0.6798, Acc : 62.5%\n",
      "Epoch [384/500], Step [2/4], Loss: 0.6393, Acc : 60.9375%\n",
      "Epoch [384/500], Step [4/4], Loss: 0.6585, Acc : 62.5%\n",
      "Epoch [385/500], Step [2/4], Loss: 0.6531, Acc : 65.625%\n",
      "Epoch [385/500], Step [4/4], Loss: 0.6661, Acc : 62.5%\n",
      "Epoch [386/500], Step [2/4], Loss: 0.6467, Acc : 62.5%\n",
      "Epoch [386/500], Step [4/4], Loss: 0.6341, Acc : 62.5%\n",
      "Epoch [387/500], Step [2/4], Loss: 0.6362, Acc : 67.1875%\n",
      "Epoch [387/500], Step [4/4], Loss: 0.5672, Acc : 100.0%\n",
      "Epoch [388/500], Step [2/4], Loss: 0.6720, Acc : 53.125%\n",
      "Epoch [388/500], Step [4/4], Loss: 0.6740, Acc : 50.0%\n",
      "Epoch [389/500], Step [2/4], Loss: 0.6579, Acc : 53.125%\n",
      "Epoch [389/500], Step [4/4], Loss: 0.6548, Acc : 50.0%\n",
      "Epoch [390/500], Step [2/4], Loss: 0.6420, Acc : 62.5%\n",
      "Epoch [390/500], Step [4/4], Loss: 0.6290, Acc : 62.5%\n",
      "Epoch [391/500], Step [2/4], Loss: 0.6697, Acc : 60.9375%\n",
      "Epoch [391/500], Step [4/4], Loss: 0.6670, Acc : 87.5%\n",
      "Epoch [392/500], Step [2/4], Loss: 0.6720, Acc : 57.8125%\n",
      "Epoch [392/500], Step [4/4], Loss: 0.6033, Acc : 100.0%\n",
      "Epoch [393/500], Step [2/4], Loss: 0.6430, Acc : 68.75%\n",
      "Epoch [393/500], Step [4/4], Loss: 0.6469, Acc : 62.5%\n",
      "Epoch [394/500], Step [2/4], Loss: 0.6569, Acc : 64.0625%\n",
      "Epoch [394/500], Step [4/4], Loss: 0.5234, Acc : 87.5%\n",
      "Epoch [395/500], Step [2/4], Loss: 0.6904, Acc : 56.25%\n",
      "Epoch [395/500], Step [4/4], Loss: 0.6971, Acc : 50.0%\n",
      "Epoch [396/500], Step [2/4], Loss: 0.6499, Acc : 65.625%\n",
      "Epoch [396/500], Step [4/4], Loss: 0.6280, Acc : 87.5%\n",
      "Epoch [397/500], Step [2/4], Loss: 0.6657, Acc : 60.9375%\n",
      "Epoch [397/500], Step [4/4], Loss: 0.6135, Acc : 62.5%\n",
      "Epoch [398/500], Step [2/4], Loss: 0.6405, Acc : 64.0625%\n",
      "Epoch [398/500], Step [4/4], Loss: 0.7153, Acc : 37.5%\n",
      "Epoch [399/500], Step [2/4], Loss: 0.6482, Acc : 64.0625%\n",
      "Epoch [399/500], Step [4/4], Loss: 0.6969, Acc : 50.0%\n",
      "Epoch [400/500], Step [2/4], Loss: 0.6405, Acc : 67.1875%\n",
      "Epoch [400/500], Step [4/4], Loss: 0.7143, Acc : 50.0%\n",
      "Epoch [401/500], Step [2/4], Loss: 0.6577, Acc : 60.9375%\n",
      "Epoch [401/500], Step [4/4], Loss: 0.6634, Acc : 62.5%\n",
      "Epoch [402/500], Step [2/4], Loss: 0.6654, Acc : 56.25%\n",
      "Epoch [402/500], Step [4/4], Loss: 0.6125, Acc : 75.0%\n",
      "Epoch [403/500], Step [2/4], Loss: 0.6488, Acc : 60.9375%\n",
      "Epoch [403/500], Step [4/4], Loss: 0.5820, Acc : 87.5%\n",
      "Epoch [404/500], Step [2/4], Loss: 0.6682, Acc : 64.0625%\n",
      "Epoch [404/500], Step [4/4], Loss: 0.6825, Acc : 50.0%\n",
      "Epoch [405/500], Step [2/4], Loss: 0.6833, Acc : 59.375%\n",
      "Epoch [405/500], Step [4/4], Loss: 0.6899, Acc : 50.0%\n",
      "Epoch [406/500], Step [2/4], Loss: 0.6456, Acc : 60.9375%\n",
      "Epoch [406/500], Step [4/4], Loss: 0.5958, Acc : 75.0%\n",
      "Epoch [407/500], Step [2/4], Loss: 0.6376, Acc : 70.3125%\n",
      "Epoch [407/500], Step [4/4], Loss: 0.7188, Acc : 37.5%\n",
      "Epoch [408/500], Step [2/4], Loss: 0.6456, Acc : 62.5%\n",
      "Epoch [408/500], Step [4/4], Loss: 0.7339, Acc : 37.5%\n",
      "Epoch [409/500], Step [2/4], Loss: 0.6605, Acc : 56.25%\n",
      "Epoch [409/500], Step [4/4], Loss: 0.7112, Acc : 50.0%\n",
      "Epoch [410/500], Step [2/4], Loss: 0.6570, Acc : 59.375%\n",
      "Epoch [410/500], Step [4/4], Loss: 0.6788, Acc : 50.0%\n",
      "Epoch [411/500], Step [2/4], Loss: 0.6359, Acc : 71.875%\n",
      "Epoch [411/500], Step [4/4], Loss: 0.6640, Acc : 62.5%\n",
      "Epoch [412/500], Step [2/4], Loss: 0.6531, Acc : 59.375%\n",
      "Epoch [412/500], Step [4/4], Loss: 0.6533, Acc : 62.5%\n",
      "Epoch [413/500], Step [2/4], Loss: 0.6352, Acc : 62.5%\n",
      "Epoch [413/500], Step [4/4], Loss: 0.5694, Acc : 87.5%\n",
      "Epoch [414/500], Step [2/4], Loss: 0.6394, Acc : 67.1875%\n",
      "Epoch [414/500], Step [4/4], Loss: 0.6885, Acc : 50.0%\n",
      "Epoch [415/500], Step [2/4], Loss: 0.6284, Acc : 70.3125%\n",
      "Epoch [415/500], Step [4/4], Loss: 0.6922, Acc : 50.0%\n",
      "Epoch [416/500], Step [2/4], Loss: 0.6484, Acc : 60.9375%\n",
      "Epoch [416/500], Step [4/4], Loss: 0.7074, Acc : 50.0%\n",
      "Epoch [417/500], Step [2/4], Loss: 0.6322, Acc : 65.625%\n",
      "Epoch [417/500], Step [4/4], Loss: 0.7021, Acc : 37.5%\n",
      "Epoch [418/500], Step [2/4], Loss: 0.6666, Acc : 64.0625%\n",
      "Epoch [418/500], Step [4/4], Loss: 0.7070, Acc : 37.5%\n",
      "Epoch [419/500], Step [2/4], Loss: 0.6495, Acc : 60.9375%\n",
      "Epoch [419/500], Step [4/4], Loss: 0.6314, Acc : 75.0%\n",
      "Epoch [420/500], Step [2/4], Loss: 0.6415, Acc : 65.625%\n",
      "Epoch [420/500], Step [4/4], Loss: 0.6186, Acc : 62.5%\n",
      "Epoch [421/500], Step [2/4], Loss: 0.6776, Acc : 57.8125%\n",
      "Epoch [421/500], Step [4/4], Loss: 0.5781, Acc : 75.0%\n",
      "Epoch [422/500], Step [2/4], Loss: 0.6985, Acc : 54.6875%\n",
      "Epoch [422/500], Step [4/4], Loss: 0.6188, Acc : 75.0%\n",
      "Epoch [423/500], Step [2/4], Loss: 0.6146, Acc : 71.875%\n",
      "Epoch [423/500], Step [4/4], Loss: 0.6424, Acc : 62.5%\n",
      "Epoch [424/500], Step [2/4], Loss: 0.6662, Acc : 50.0%\n",
      "Epoch [424/500], Step [4/4], Loss: 0.7050, Acc : 37.5%\n",
      "Epoch [425/500], Step [2/4], Loss: 0.6671, Acc : 57.8125%\n",
      "Epoch [425/500], Step [4/4], Loss: 0.5984, Acc : 62.5%\n",
      "Epoch [426/500], Step [2/4], Loss: 0.6500, Acc : 60.9375%\n",
      "Epoch [426/500], Step [4/4], Loss: 0.6604, Acc : 62.5%\n",
      "Epoch [427/500], Step [2/4], Loss: 0.6766, Acc : 60.9375%\n",
      "Epoch [427/500], Step [4/4], Loss: 0.5863, Acc : 75.0%\n",
      "Epoch [428/500], Step [2/4], Loss: 0.6182, Acc : 67.1875%\n",
      "Epoch [428/500], Step [4/4], Loss: 0.6768, Acc : 62.5%\n",
      "Epoch [429/500], Step [2/4], Loss: 0.6405, Acc : 64.0625%\n",
      "Epoch [429/500], Step [4/4], Loss: 0.5784, Acc : 87.5%\n",
      "Epoch [430/500], Step [2/4], Loss: 0.6432, Acc : 60.9375%\n",
      "Epoch [430/500], Step [4/4], Loss: 0.6846, Acc : 50.0%\n",
      "Epoch [431/500], Step [2/4], Loss: 0.6835, Acc : 48.4375%\n",
      "Epoch [431/500], Step [4/4], Loss: 0.5856, Acc : 87.5%\n",
      "Epoch [432/500], Step [2/4], Loss: 0.6386, Acc : 60.9375%\n",
      "Epoch [432/500], Step [4/4], Loss: 0.5522, Acc : 87.5%\n",
      "Epoch [433/500], Step [2/4], Loss: 0.6360, Acc : 62.5%\n",
      "Epoch [433/500], Step [4/4], Loss: 0.6739, Acc : 50.0%\n",
      "Epoch [434/500], Step [2/4], Loss: 0.6632, Acc : 54.6875%\n",
      "Epoch [434/500], Step [4/4], Loss: 0.7683, Acc : 12.5%\n",
      "Epoch [435/500], Step [2/4], Loss: 0.6555, Acc : 65.625%\n",
      "Epoch [435/500], Step [4/4], Loss: 0.5781, Acc : 87.5%\n",
      "Epoch [436/500], Step [2/4], Loss: 0.6453, Acc : 70.3125%\n",
      "Epoch [436/500], Step [4/4], Loss: 0.6499, Acc : 62.5%\n",
      "Epoch [437/500], Step [2/4], Loss: 0.6312, Acc : 70.3125%\n",
      "Epoch [437/500], Step [4/4], Loss: 0.6190, Acc : 62.5%\n",
      "Epoch [438/500], Step [2/4], Loss: 0.6389, Acc : 68.75%\n",
      "Epoch [438/500], Step [4/4], Loss: 0.5761, Acc : 75.0%\n",
      "Epoch [439/500], Step [2/4], Loss: 0.6489, Acc : 59.375%\n",
      "Epoch [439/500], Step [4/4], Loss: 0.6045, Acc : 75.0%\n",
      "Epoch [440/500], Step [2/4], Loss: 0.6459, Acc : 65.625%\n",
      "Epoch [440/500], Step [4/4], Loss: 0.5617, Acc : 62.5%\n",
      "Epoch [441/500], Step [2/4], Loss: 0.6429, Acc : 67.1875%\n",
      "Epoch [441/500], Step [4/4], Loss: 0.6591, Acc : 50.0%\n",
      "Epoch [442/500], Step [2/4], Loss: 0.6279, Acc : 64.0625%\n",
      "Epoch [442/500], Step [4/4], Loss: 0.6314, Acc : 62.5%\n",
      "Epoch [443/500], Step [2/4], Loss: 0.6508, Acc : 60.9375%\n",
      "Epoch [443/500], Step [4/4], Loss: 0.6260, Acc : 75.0%\n",
      "Epoch [444/500], Step [2/4], Loss: 0.6496, Acc : 56.25%\n",
      "Epoch [444/500], Step [4/4], Loss: 0.6854, Acc : 62.5%\n",
      "Epoch [445/500], Step [2/4], Loss: 0.6365, Acc : 65.625%\n",
      "Epoch [445/500], Step [4/4], Loss: 0.6204, Acc : 62.5%\n",
      "Epoch [446/500], Step [2/4], Loss: 0.6498, Acc : 65.625%\n",
      "Epoch [446/500], Step [4/4], Loss: 0.6247, Acc : 75.0%\n",
      "Epoch [447/500], Step [2/4], Loss: 0.6578, Acc : 57.8125%\n",
      "Epoch [447/500], Step [4/4], Loss: 0.7373, Acc : 50.0%\n",
      "Epoch [448/500], Step [2/4], Loss: 0.6559, Acc : 59.375%\n",
      "Epoch [448/500], Step [4/4], Loss: 0.5695, Acc : 75.0%\n",
      "Epoch [449/500], Step [2/4], Loss: 0.6311, Acc : 68.75%\n",
      "Epoch [449/500], Step [4/4], Loss: 0.7251, Acc : 50.0%\n",
      "Epoch [450/500], Step [2/4], Loss: 0.6860, Acc : 51.5625%\n",
      "Epoch [450/500], Step [4/4], Loss: 0.6630, Acc : 62.5%\n",
      "Epoch [451/500], Step [2/4], Loss: 0.6576, Acc : 62.5%\n",
      "Epoch [451/500], Step [4/4], Loss: 0.7333, Acc : 37.5%\n",
      "Epoch [452/500], Step [2/4], Loss: 0.6308, Acc : 70.3125%\n",
      "Epoch [452/500], Step [4/4], Loss: 0.6951, Acc : 50.0%\n",
      "Epoch [453/500], Step [2/4], Loss: 0.6581, Acc : 59.375%\n",
      "Epoch [453/500], Step [4/4], Loss: 0.5686, Acc : 87.5%\n",
      "Epoch [454/500], Step [2/4], Loss: 0.6326, Acc : 71.875%\n",
      "Epoch [454/500], Step [4/4], Loss: 0.6276, Acc : 62.5%\n",
      "Epoch [455/500], Step [2/4], Loss: 0.6372, Acc : 71.875%\n",
      "Epoch [455/500], Step [4/4], Loss: 0.5638, Acc : 87.5%\n",
      "Epoch [456/500], Step [2/4], Loss: 0.6411, Acc : 65.625%\n",
      "Epoch [456/500], Step [4/4], Loss: 0.6486, Acc : 75.0%\n",
      "Epoch [457/500], Step [2/4], Loss: 0.6722, Acc : 54.6875%\n",
      "Epoch [457/500], Step [4/4], Loss: 0.6390, Acc : 75.0%\n",
      "Epoch [458/500], Step [2/4], Loss: 0.6399, Acc : 59.375%\n",
      "Epoch [458/500], Step [4/4], Loss: 0.6520, Acc : 50.0%\n",
      "Epoch [459/500], Step [2/4], Loss: 0.6332, Acc : 68.75%\n",
      "Epoch [459/500], Step [4/4], Loss: 0.5602, Acc : 75.0%\n",
      "Epoch [460/500], Step [2/4], Loss: 0.6317, Acc : 65.625%\n",
      "Epoch [460/500], Step [4/4], Loss: 0.6698, Acc : 62.5%\n",
      "Epoch [461/500], Step [2/4], Loss: 0.6230, Acc : 75.0%\n",
      "Epoch [461/500], Step [4/4], Loss: 0.6159, Acc : 75.0%\n",
      "Epoch [462/500], Step [2/4], Loss: 0.6419, Acc : 71.875%\n",
      "Epoch [462/500], Step [4/4], Loss: 0.7655, Acc : 37.5%\n",
      "Epoch [463/500], Step [2/4], Loss: 0.6523, Acc : 62.5%\n",
      "Epoch [463/500], Step [4/4], Loss: 0.6563, Acc : 50.0%\n",
      "Epoch [464/500], Step [2/4], Loss: 0.6675, Acc : 57.8125%\n",
      "Epoch [464/500], Step [4/4], Loss: 0.6855, Acc : 62.5%\n",
      "Epoch [465/500], Step [2/4], Loss: 0.6345, Acc : 65.625%\n",
      "Epoch [465/500], Step [4/4], Loss: 0.6540, Acc : 50.0%\n",
      "Epoch [466/500], Step [2/4], Loss: 0.6314, Acc : 70.3125%\n",
      "Epoch [466/500], Step [4/4], Loss: 0.7013, Acc : 50.0%\n",
      "Epoch [467/500], Step [2/4], Loss: 0.6410, Acc : 60.9375%\n",
      "Epoch [467/500], Step [4/4], Loss: 0.6755, Acc : 62.5%\n",
      "Epoch [468/500], Step [2/4], Loss: 0.6668, Acc : 59.375%\n",
      "Epoch [468/500], Step [4/4], Loss: 0.5649, Acc : 75.0%\n",
      "Epoch [469/500], Step [2/4], Loss: 0.6672, Acc : 56.25%\n",
      "Epoch [469/500], Step [4/4], Loss: 0.5958, Acc : 75.0%\n",
      "Epoch [470/500], Step [2/4], Loss: 0.6525, Acc : 60.9375%\n",
      "Epoch [470/500], Step [4/4], Loss: 0.7014, Acc : 50.0%\n",
      "Epoch [471/500], Step [2/4], Loss: 0.6291, Acc : 73.4375%\n",
      "Epoch [471/500], Step [4/4], Loss: 0.6100, Acc : 75.0%\n",
      "Epoch [472/500], Step [2/4], Loss: 0.6389, Acc : 70.3125%\n",
      "Epoch [472/500], Step [4/4], Loss: 0.6141, Acc : 62.5%\n",
      "Epoch [473/500], Step [2/4], Loss: 0.6461, Acc : 62.5%\n",
      "Epoch [473/500], Step [4/4], Loss: 0.5912, Acc : 75.0%\n",
      "Epoch [474/500], Step [2/4], Loss: 0.5936, Acc : 78.125%\n",
      "Epoch [474/500], Step [4/4], Loss: 0.7759, Acc : 37.5%\n",
      "Epoch [475/500], Step [2/4], Loss: 0.6698, Acc : 54.6875%\n",
      "Epoch [475/500], Step [4/4], Loss: 0.6583, Acc : 62.5%\n",
      "Epoch [476/500], Step [2/4], Loss: 0.6443, Acc : 64.0625%\n",
      "Epoch [476/500], Step [4/4], Loss: 0.5975, Acc : 87.5%\n",
      "Epoch [477/500], Step [2/4], Loss: 0.6535, Acc : 62.5%\n",
      "Epoch [477/500], Step [4/4], Loss: 0.7009, Acc : 62.5%\n",
      "Epoch [478/500], Step [2/4], Loss: 0.6529, Acc : 62.5%\n",
      "Epoch [478/500], Step [4/4], Loss: 0.5536, Acc : 62.5%\n",
      "Epoch [479/500], Step [2/4], Loss: 0.6462, Acc : 62.5%\n",
      "Epoch [479/500], Step [4/4], Loss: 0.7090, Acc : 50.0%\n",
      "Epoch [480/500], Step [2/4], Loss: 0.6396, Acc : 64.0625%\n",
      "Epoch [480/500], Step [4/4], Loss: 0.6767, Acc : 50.0%\n",
      "Epoch [481/500], Step [2/4], Loss: 0.6246, Acc : 70.3125%\n",
      "Epoch [481/500], Step [4/4], Loss: 0.5863, Acc : 75.0%\n",
      "Epoch [482/500], Step [2/4], Loss: 0.6177, Acc : 71.875%\n",
      "Epoch [482/500], Step [4/4], Loss: 0.6688, Acc : 50.0%\n",
      "Epoch [483/500], Step [2/4], Loss: 0.6428, Acc : 67.1875%\n",
      "Epoch [483/500], Step [4/4], Loss: 0.7469, Acc : 50.0%\n",
      "Epoch [484/500], Step [2/4], Loss: 0.6245, Acc : 71.875%\n",
      "Epoch [484/500], Step [4/4], Loss: 0.5477, Acc : 75.0%\n",
      "Epoch [485/500], Step [2/4], Loss: 0.6410, Acc : 64.0625%\n",
      "Epoch [485/500], Step [4/4], Loss: 0.8561, Acc : 0.0%\n",
      "Epoch [486/500], Step [2/4], Loss: 0.6425, Acc : 65.625%\n",
      "Epoch [486/500], Step [4/4], Loss: 0.6896, Acc : 50.0%\n",
      "Epoch [487/500], Step [2/4], Loss: 0.6645, Acc : 56.25%\n",
      "Epoch [487/500], Step [4/4], Loss: 0.6523, Acc : 62.5%\n",
      "Epoch [488/500], Step [2/4], Loss: 0.6390, Acc : 57.8125%\n",
      "Epoch [488/500], Step [4/4], Loss: 0.6245, Acc : 87.5%\n",
      "Epoch [489/500], Step [2/4], Loss: 0.6719, Acc : 54.6875%\n",
      "Epoch [489/500], Step [4/4], Loss: 0.6284, Acc : 75.0%\n",
      "Epoch [490/500], Step [2/4], Loss: 0.6195, Acc : 68.75%\n",
      "Epoch [490/500], Step [4/4], Loss: 0.6486, Acc : 75.0%\n",
      "Epoch [491/500], Step [2/4], Loss: 0.6407, Acc : 64.0625%\n",
      "Epoch [491/500], Step [4/4], Loss: 0.6788, Acc : 50.0%\n",
      "Epoch [492/500], Step [2/4], Loss: 0.6256, Acc : 62.5%\n",
      "Epoch [492/500], Step [4/4], Loss: 0.7805, Acc : 25.0%\n",
      "Epoch [493/500], Step [2/4], Loss: 0.6579, Acc : 59.375%\n",
      "Epoch [493/500], Step [4/4], Loss: 0.5946, Acc : 87.5%\n",
      "Epoch [494/500], Step [2/4], Loss: 0.6409, Acc : 64.0625%\n",
      "Epoch [494/500], Step [4/4], Loss: 0.6357, Acc : 62.5%\n",
      "Epoch [495/500], Step [2/4], Loss: 0.6404, Acc : 64.0625%\n",
      "Epoch [495/500], Step [4/4], Loss: 0.6851, Acc : 62.5%\n",
      "Epoch [496/500], Step [2/4], Loss: 0.6708, Acc : 51.5625%\n",
      "Epoch [496/500], Step [4/4], Loss: 0.6960, Acc : 50.0%\n",
      "Epoch [497/500], Step [2/4], Loss: 0.6310, Acc : 68.75%\n",
      "Epoch [497/500], Step [4/4], Loss: 0.6584, Acc : 50.0%\n",
      "Epoch [498/500], Step [2/4], Loss: 0.6459, Acc : 60.9375%\n",
      "Epoch [498/500], Step [4/4], Loss: 0.6328, Acc : 75.0%\n",
      "Epoch [499/500], Step [2/4], Loss: 0.6434, Acc : 57.8125%\n",
      "Epoch [499/500], Step [4/4], Loss: 0.6919, Acc : 62.5%\n",
      "Epoch [500/500], Step [2/4], Loss: 0.6385, Acc : 67.1875%\n",
      "Epoch [500/500], Step [4/4], Loss: 0.6201, Acc : 50.0%\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "data = dataiter.next()\n",
    "features,labels = data\n",
    "\n",
    "class StutterNet(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(StutterNet, self).__init__()\n",
    "        # input shape = (batch_size, 1, 149,1024)\n",
    "        # in_channels is batch size\n",
    "        self.layer1 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            #torch.nn.Dropout(p=0.5)\n",
    "        )\n",
    "        # input size = (batch_size, 8, 74, 512)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 8, out_channels = 16, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "            #torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        # input size = (batch_size, 16, 37, 256)\n",
    "        self.layer3 = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "            #torch.nn.Dropout(p=0.25)\n",
    "        )\n",
    "        # # input size = (batch_size, 32, 19, 128)\n",
    "        # self.layer4 = nn.Sequential(\n",
    "        #     torch.nn.Conv2d(in_channels = 32, out_channels = 16, kernel_size=3, stride=1, padding=1),\n",
    "        #     torch.nn.ReLU(),\n",
    "        #     torch.nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        #     #torch.nn.Dropout(p=0.5)\n",
    "        # )\n",
    "        # input size = (batch_size, 16, 10, 64)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*19*128,2, bias=True)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #print('Before Layer1',np.shape(x))\n",
    "        out = self.layer1(x)\n",
    "        #print('After layer 1',np.shape(out))\n",
    "        out = self.layer2(out)\n",
    "        #print('After layer 2',np.shape(out))\n",
    "        out = self.layer3(out)\n",
    "        #print('After layer 3',np.shape(out))\n",
    "        #out = self.layer4(out)\n",
    "        out  = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        #print('After final ',np.shape(out))\n",
    "\n",
    "        log_probs = torch.nn.functional.log_softmax(out, dim=1)\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "model = StutterNet(batch_size).to(device)\n",
    "print(model)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#weighted loss\n",
    "#criterion = nn.CrossEntropyLoss(weight = torch.FloatTensor([1, 3]).to(device)) # Class 0 is 75% of the total dataset \n",
    "#criterion = nn.LogSoftmax()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the model\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    n_correct = 0\n",
    "    for i, (features, labels) in enumerate(train_loader):  \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = torch.reshape(labels,(np.shape(labels)[0],))\n",
    "        labels = labels.to(torch.int64)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute Training Accuracy\n",
    "        _, predicted_labels = torch.max(outputs.data, 1)\n",
    "        n_correct = (labels == predicted_labels).sum()\n",
    "        acc = 100.0 * n_correct / outputs.shape[0]\n",
    "        # visualisation\n",
    "        writer.add_scalar(\"Loss/train\", loss, epoch)  \n",
    "        writer.add_scalar(\"Accuracy/train\", acc, epoch)  \n",
    "        \n",
    "        if (i+1) % 2 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}, Acc : {acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1])\n",
      "torch.Size([64, 1, 149, 1024])\n",
      "torch.Size([64, 2])\n",
      "tensor([1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0], device='cuda:0')\n",
      "torch.Size([64])\n",
      "0\n",
      "tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
      "        0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1])\n",
      "torch.Size([64, 1, 149, 1024])\n",
      "torch.Size([64, 2])\n",
      "tensor([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([64])\n",
      "38\n",
      "tensor([0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n",
      "        0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0])\n",
      "torch.Size([64, 1, 149, 1024])\n",
      "torch.Size([64, 2])\n",
      "tensor([0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "torch.Size([64])\n",
      "71\n",
      "tensor([0, 1, 0, 0, 0, 1])\n",
      "torch.Size([6, 1, 149, 1024])\n",
      "torch.Size([6, 2])\n",
      "tensor([0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([6])\n",
      "105\n",
      "Accuracy of the network on test dataset is : 55.05050505050505 %\n",
      "Precision of the network on test dataset is : 0.5595238095238095\n",
      "Recall of the network on test dataset is : 0.47474747474747475\n",
      "F1 Score of the network on test dataset is : 0.5136612021857925\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "   n_correct = 0\n",
    "   # Compute F1 score, precision and recall\n",
    "   predicted_stutter = 0\n",
    "   labels_stutter = 0\n",
    "   correct_stutter = 0\n",
    "   i = 0\n",
    "   final_label = []\n",
    "   final_predicted = []\n",
    "   for features, labels in test_loader:\n",
    "       print(labels)\n",
    "       print(np.shape(features))\n",
    "       features = features.to(device)\n",
    "       labels = labels.to(device)\n",
    "       outputs = model(features)\n",
    "       print(np.shape(outputs))\n",
    "       # max returns (value ,index)\n",
    "       _, predicted = torch.max(outputs.data, 1)\n",
    "       print(predicted)\n",
    "       print(np.shape(predicted))\n",
    "       label = torch.transpose(predicted, -1, 0)\n",
    "       predicted = torch.reshape(predicted,(outputs.shape[0],1))\n",
    "       i = i+1\n",
    "       print(n_correct)\n",
    "       final_label.append(label)\n",
    "       final_predicted.append(predicted)\n",
    "       for i in range (0, outputs.shape[0]) :\n",
    "               # F1 score for stutter\n",
    "            if (predicted[i] == 1) :\n",
    "                predicted_stutter +=1\n",
    "            if (labels[i] == 1) :\n",
    "                labels_stutter +=1   \n",
    "            if ((predicted[i] == 1) & (labels[i] == 1)):\n",
    "                correct_stutter +=1\n",
    "            if (predicted[i] == labels[i]) :\n",
    "                n_correct = n_correct + 1\n",
    "\n",
    "\n",
    "\n",
    "acc_test = 100*n_correct/n_samples_test\n",
    "print(f'Accuracy of the network on test dataset is : {acc_test} %')\n",
    "recall = correct_stutter/ labels_stutter\n",
    "precision = correct_stutter / predicted_stutter\n",
    "f1_score = 2 * precision * recall / (precision + recall)    \n",
    "print(f'Precision of the network on test dataset is : {precision}')\n",
    "print(f'Recall of the network on test dataset is : {recall}')\n",
    "print(f'F1 Score of the network on test dataset is : {f1_score}')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0], device='cuda:0'), tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
      "        0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1], device='cuda:0'), tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0], device='cuda:0'), tensor([0, 0, 0, 1, 0, 0], device='cuda:0')]\n",
      "-------------------------------------\n",
      "Predicted\n",
      "[tensor([[0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0]], device='cuda:0'), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1]], device='cuda:0'), tensor([[1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0'), tensor([[0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0]], device='cuda:0')]\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(final_label)\n",
    "print('-------------------------------------')\n",
    "print('Predicted')\n",
    "print(final_predicted)\n",
    "print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aede6801b29297ca4753447bcd9fb52ef6084259d335c2409ba40d2da343a20"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
